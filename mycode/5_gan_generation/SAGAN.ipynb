{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANによる画像生成\n",
    "\n",
    "commit用\n",
    "\n",
    "```mycode/5_gan_generation/```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: gpustat: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-3 Self-Attention GANの概要\n",
    "\n",
    "2018年時点で最高峰のGANの１つである**BigGAN**もSAGANをベースとしているGANである．本節では\n",
    "\n",
    "- Self-Attention\n",
    "- pointwise convolution\n",
    "- Spectral Normalization\n",
    "\n",
    "の３つのテーマについての理解を目標とする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 従来のGANの問題点\n",
    "従来のGANの転置畳み込みを繰り返す操作は特徴量マップが大きくなりますが，転置畳み込みの繰り返しでは**局所的な情報の拡大にしかならない**問題がある．\n",
    "\n",
    "よってより良い画像生成を実現するためには可能であれば拡大する際に画像全体の大域的な情報を考慮する仕組みが必要になる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attentionの導入\n",
    "従来のニューラルネットワークでは前の層の出力がそのまま次の層に入力されるので式としては\n",
    "$$\n",
    "y = x\n",
    "$$\n",
    "で表される．しかしこれでは局所的な性質が常に伝わっていくので，さらに大域的な情報を用いて表される$o$を導入する．この$o$に係数$\\gamma$をかけた値を足して次の層に伝達することを考える．\n",
    "$$\n",
    "y = x +\\gamma o\n",
    "$$\n",
    "この$o$をどのように求めるかについては以下の手順で求める．\n",
    "1. $x$を[C,W,H]から[C,N]に変形する. $N=W\\times H$\n",
    "2. $S=x^Tx$ という行列を用意する．これは画像位置$i$と画像位置$j$の関係性を表す行列である．\n",
    "3. $S$を行方向にソフトマックス関数にかける．これで画像位置$i$と画像位置$j$の関係性の総和が1になり扱いやすくなる．これを$\\beta$とし，これを**Attention Map**という．\n",
    "4. 上で作った$\\beta$を$x$にかけることで$o$を得る．つまり$o = x\\beta^T$である．$o=[C,N]$\n",
    "\n",
    "この$o = x\\beta^T$について\n",
    "$$\n",
    "o_{c=k,n=j} = \\sum_{i=1}^Nx_{ki}\\beta_{ji}\n",
    "$$\n",
    "この式はチャネル$k$において位置$j$について位置$i=1,2,\\cdots N$の全ての影響を総和することを意味している．\n",
    "$x_{ki}$ はチャネル$k$の位置$i$の特徴量を表していて，$\\beta_{ji}$は位置$j$と位置$i$の関係性を表して，この$\\sum$はそれらの積の総和を求めている．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　最初のサイズ変形 (B,C,W,H)  to (B,C,N)\n",
    "X = torch.randn(3,32,64,64)\n",
    "X = X.view(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "# 掛け算\n",
    "X_T = X.permute(0,2,1)\n",
    "S = torch.bmm(X_T, X)\n",
    "\n",
    "# 規格化\n",
    "m = nn.Softmax(dim=-2)\n",
    "attention_map_T = m(S)\n",
    "attention_map = attention_map_T.permute(0,2,1)\n",
    "\n",
    "# Self-Attention Map\n",
    "o = torch.bmm(X, attention_map.permute(0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1×1 Convolutions (pointwise convolution)\n",
    "Self-Attention の制限は非常に強いのでこのままでは学習はうまくいかないことが多い．\n",
    "そこでpointwise convolutionと呼ばれる前処理をした特徴量マップをSelf-Attentionに渡してあげることによって学習がうまくいくようにする．\n",
    "またこれをすることによって特徴量マップのチャネル数を変えることができる(一般にはチャネル数を減らす)ので計算量を調整することができる．\n",
    "\n",
    "またpointwise convolutionでは次の言葉が使われる\n",
    "- query：元の入力$x$の転置に対応するもの\n",
    "- key：元の入力$x$に対応するもの\n",
    "- value：Attention Mapと掛け算する対象\n",
    "\n",
    "出力のチャネルはカーネルの数となる．カーネルのチャネルは入力のチャネルに合わせる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(3,32,64,64)\n",
    "\n",
    "# 1×1の畳み込み層によるpointwise convolutionを用意　\n",
    "query_conv = nn.Conv2d(\n",
    "    in_channels=X.shape[1], out_channels=X.shape[1]//8, kernel_size=1)\n",
    "\n",
    "key_conv = nn.Conv2d(\n",
    "    in_channels=X.shape[1], out_channels=X.shape[1]//8, kernel_size=1)\n",
    "\n",
    "value_conv = nn.Conv2d(\n",
    "    in_channels=X.shape[1], out_channels=X.shape[1], kernel_size=1)\n",
    "\n",
    "# 畳み込みをしてからサイズを変更する\n",
    "proj_query = query_conv(X).view(\n",
    "    X.shape[0], -1, X.shape[2]*X.shape[3])\n",
    "proj_query = proj_query.permute(0,2,1)\n",
    "proj_key = key_conv(X).view(\n",
    "    X.shape[0], -1, X.shape[2]*X.shape[3])\n",
    "\n",
    "# 掛け算\n",
    "S = torch.bmm(proj_query, proj_key) # bmmはバッチごとの掛け算を行う\n",
    "\n",
    "# 規格化\n",
    "m = nn.Softmax(dim=-2)\n",
    "attention_map_T = m(S)\n",
    "attention_map = attention_map_T.permute(0,2,1)\n",
    "\n",
    "# Self-Attention Map\n",
    "proj_value = value_conv(X).view(\n",
    "    X.shape[0], -1, X.shape[2]*X.shape[3])\n",
    "o = torch.bmm(proj_value, attention_map.permute(0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Normalization\n",
    "- Spectral Normalization：畳み込みの重みに対する規格化\n",
    "- Batch Normalization：ディープラーニングモデルを流れるデータに対する規格化\n",
    "\n",
    "***リプシッツ連続性(Lipschitz continuity)：***判別機Dの出力が入力データに対して頑健性を持つこと\n",
    "具体的には層を表す行列の固有値のうち最大の固有値で全ての重みを割ることで規格化している．固有値は固有ベクトルの拡大率なので，一番拡大される固有値で全てを規格化することによって流れるデータが想定外に大きくなることを防ぐ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvTranspose2d(20, 512, kernel_size=(4, 4), stride=(1, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_dim = 20\n",
    "image_size = 64\n",
    "nn.utils.spectral_norm(nn.ConvTranspose2d(\n",
    "    z_dim, image_size*8, kernel_size=4,stride=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-4 Self-Attention GANの学習，生成の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attentionモジュールの実装\n",
    "$$\n",
    "y = x + \\gamma o\n",
    "$$\n",
    "$\\gamma$ は初期値０から始めるが，学習させる対象なのでこれも```nn.Parameter()```を用いて実装する．\n",
    "\n",
    "Attention MapはあとでAttentionの強さを可視化するために出力させてる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(nn.Module):\n",
    "    \"\"\" Self-AttentionのLayer\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim):\n",
    "        super(Self_Attention, self).__init__()\n",
    "        \n",
    "        # 1×1の畳み込み層によるpointwise convolutionを用意　\n",
    "        self.query_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "\n",
    "        self.key_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "\n",
    "        self.value_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        \n",
    "        # 規格化\n",
    "        self.softmax = nn.Softmax(dim=-2)\n",
    "        \n",
    "        # gamma\n",
    "        # 学習させるのでnn.Parameter　を用いる\n",
    "        self.gamma = nn.Parameter(torch.zero(1))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 入力変数\n",
    "        X = x\n",
    "        \n",
    "        # 畳み込みをしてからサイズを変更する\n",
    "        proj_query = self.query_conv(X).view(X.shape[0], -1, X.shape[2]*X.shape[3])\n",
    "        proj_query = self.proj_query.permute(0,2,1)\n",
    "        proj_key = self.key_conv(X).view(X.shape[0], -1, X.shape[2]*X.shape[3])\n",
    "        \n",
    "        \n",
    "        # 掛け算\n",
    "        S = torch.bmm(proj_query, proj_key) # bmmはバッチごとの掛け算を行う  \n",
    "        \n",
    "        # 規格化\n",
    "        attention_map_T = self.softmax(S)\n",
    "        attention_map = attention_map_T.permute(0,2,1)\n",
    "        \n",
    "        # Self-Attention Map\n",
    "        proj_value = self.value_conv(X).view(X.shape[0], -1, X.shape[2]*X.shape[3])\n",
    "        o = torch.bmm(proj_value, attention_map.permute(0,2,1))\n",
    "        \n",
    "        # Self-Attention　MapのテンソルサイズをXに揃えて出力\n",
    "        o = o.view(X.shape[0],X.shape[1],X.shape[2],X.shape[3])\n",
    "        out = x + self.gamma*o\n",
    "        \n",
    "        return out , attention_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generatorの実装\n",
    "DCGANとの変更点は以下の2点である．\n",
    "1. 最後のlaseのLayer以外には転置畳み込み層にSpectral Normalizationを追加する．\n",
    "2. layer3-layer4と，layer4-lastの二箇所にSelf-Attentionモジュールを追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_dim = 20, image_size=64):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            #　ここでConvTranspose2dにspectral_norm を作用させる\n",
    "#             nn.ConvTranspose2d(z_dim, image_size*8, kernel_size=4, stride=1),\n",
    "            nn.utils.spectral_norm(nn.ConvTranspose2d(z_dim, image_size*8, kernel_size=4,stride=1)),\n",
    "            nn.BatchNorm2d(image_size*8),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.ConvTranspose2d(image_size*8, image_size*4, kernel_size=4,stride=2, padding=1)), #spectral_norm\n",
    "            nn.BatchNorm2d(image_size*4),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.ConvTranspose2d(image_size*4, image_size*2, kernel_size=4,stride=2, padding=1)), #spectral_norm\n",
    "            nn.BatchNorm2d(image_size*2),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Self-Attention 層\n",
    "        self.self_attention1 = Self_Attention(in_dim=image_size*2)\n",
    "              \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.ConvTranspose2d(image_size*2, image_size, kernel_size=4,stride=2, padding=1)), #spectral_norm\n",
    "            nn.BatchNorm2d(image_size),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "        # Self-Attention 層\n",
    "        self.self_attention2 = Self_Attention(in_dim=image_size)\n",
    "        \n",
    "        self.last = nn.Sequential(\n",
    "            nn.ConvTranspose2d(image_size, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh())\n",
    "        # 白黒なので出力は1チャネル\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.layer1(z)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out, attention_map1 = self.self_attention1(out)\n",
    "        out = self.layer4(out)\n",
    "        out, attention_map2 = self.self_attention2(out)\n",
    "        out = self.last(out)\n",
    "        \n",
    "        return out, attention_map1, attention_map2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminatorの実装\n",
    "DCGANからの変更点は以下の2点\n",
    "1. last以外のlayerに畳み込み層にSpectral Normalizationを追加する\n",
    "2. layer3-layer4と，layer4-lastの二箇所にSelf-Attentionモジュールを追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_dim=20, image_size=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(1, image_size, kernel_size=4, stride=2, padding=1)), # Spectral Normalizationの追加\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(image_size, image_size*2, kernel_size=4, stride=2, padding=1)),# Spectral Normalizationの追加\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(image_size*2, image_size*4, kernel_size=4, stride=2, padding=1)),# Spectral Normalizationの追加\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "        \n",
    "        # Self-Attention層の追加\n",
    "        self.self_attention1 = Self_Attention(in_dim=image_size*4)\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(image_size*4, image_size*8, kernel_size=4, stride=2, padding=1)),# Spectral Normalizationの追加\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "\n",
    "        # Self-Attention層の追加\n",
    "        self.self_attention2 = Self_Attention(in_dim=image_size*8)\n",
    "        \n",
    "        self.last = nn.Conv2d(image_size*8, 1, kernel_size=4, stride=1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out, attention_map1 = self.self_attention1(out)\n",
    "        out = self.layer4(out)\n",
    "        out, attention_map2 = self.self_attention1(out)\n",
    "        out = self.last(out)\n",
    "        \n",
    "        return out, attention_map1, attention_map2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaderの実装\n",
    "DCGANと同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datapath_list():\n",
    "    \n",
    "    train_img_list = list()\n",
    "    \n",
    "    for img_idx in range(200):\n",
    "        img_path = \"./data/img_78/img_7_\" + str(img_idx) + \".jpg\"\n",
    "        train_img_list.append(img_path)\n",
    "        \n",
    "        img_path = \"./data/img_78/img_8_\" + str(img_idx) + \".jpg\"\n",
    "        train_img_list.append(img_path)\n",
    "        \n",
    "    return train_img_list\n",
    "\n",
    "\n",
    "class ImageTransform():\n",
    "    \"\"\"\n",
    "    画像クラスの前処理\n",
    "    \"\"\"\n",
    "    def __init__(self, mean, std):\n",
    "        self.data_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        return self.data_transform(img)\n",
    "    \n",
    "\n",
    "class GAN_Img_Dataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    画像のDataset\n",
    "    \"\"\"\n",
    "    def __init__(self, file_list, transform):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\"画像の枚数を返す\"\"\"\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \"\"\"前処理をした画像のTensor形式のデータを取得\"\"\"\n",
    "        \n",
    "        img_path = self.file_list[index]\n",
    "        img = Image.open(img_path) #H,W,C\n",
    "        \n",
    "        # 前処理\n",
    "        img_transformed = self.transform(img)\n",
    "        \n",
    "        return img_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderの作成\n",
    "train_img_list = make_datapath_list()\n",
    "# print(train_img_list)\n",
    "\n",
    "# Datasetの作成\n",
    "mean = (0.5,)\n",
    "std = (0.5,)\n",
    "train_dataset = GAN_Img_Dataset(train_img_list, ImageTransform(mean, std))\n",
    "\n",
    "# DataLoaderの作成\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                              batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ネットワークの初期化と学習の実施\n",
    "SAGANの損失関数はhinge version of the adversarial lossと呼ばれるものに変更している\n",
    "$$\n",
    "-\\frac{1}{M}\\sum_{i=1}^M[l_i \\ast \\min (0, -1+y_i) + (1-l_i)\\ast \\min(0, -1-y_i)]\n",
    "$$\n",
    "これは実装では\n",
    "```d_loss_real = torch.nn.ReLU()(1.0-d_out_real).mean()``` となる．ここでReLUは活性化関数ではなくmin(正確にはmax)の部分に相当する．\n",
    "**シグモイドにはしてないがこれは？**\n",
    "\n",
    "またGの方では以下のように損失関数を改める．\n",
    "$$\n",
    "-\\frac{1}{M}\\sum_{i=1}^M D(G(z_i))\n",
    "$$\n",
    "これによって実装は\n",
    "```g_loss = - d_out_fake.mean()``` となる．\n",
    "\n",
    "このような損失関数が用いられているのは経験的にうまくいくからである．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
