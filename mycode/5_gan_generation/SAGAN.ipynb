{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANによる画像生成\n",
    "\n",
    "commit用\n",
    "\n",
    "```mycode/5_gan_generation/```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: gpustat: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-3 Self-Attention GANの概要\n",
    "\n",
    "2018年時点で最高峰のGANの１つである**BigGAN**もSAGANをベースとしているGANである．本節では\n",
    "\n",
    "- Self-Attention\n",
    "- pointwise convolution\n",
    "- Spectral Normalization\n",
    "\n",
    "の３つのテーマについての理解を目標とする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 従来のGANの問題点\n",
    "従来のGANの転置畳み込みを繰り返す操作は特徴量マップが大きくなりますが，転置畳み込みの繰り返しでは**局所的な情報の拡大にしかならない**問題がある．\n",
    "\n",
    "よってより良い画像生成を実現するためには可能であれば拡大する際に画像全体の大域的な情報を考慮する仕組みが必要になる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attentionの導入\n",
    "従来のニューラルネットワークでは前の層の出力がそのまま次の層に入力されるので式としては\n",
    "$$\n",
    "y = x\n",
    "$$\n",
    "で表される．しかしこれでは局所的な性質が常に伝わっていくので，さらに大域的な情報を用いて表される$o$を導入する．この$o$に係数$\\gamma$をかけた値を足して次の層に伝達することを考える．\n",
    "$$\n",
    "y = x +\\gamma o\n",
    "$$\n",
    "この$o$をどのように求めるかについては以下の手順で求める．\n",
    "1. $x$を[C,W,H]から[C,N]に変形する. $N=W\\times H$\n",
    "2. $S=x^Tx$ という行列を用意する．これは画像位置$i$と画像位置$j$の関係性を表す行列である．\n",
    "3. $S$を行方向にソフトマックス関数にかける．これで画像位置$i$と画像位置$j$の関係性の総和が1になり扱いやすくなる．これを$\\beta$とし，これを**Attention Map**という．\n",
    "4. 上で作った$\\beta$を$x$にかけることで$o$を得る．つまり$o = x\\beta^T$である．$o=[C,N]$\n",
    "\n",
    "この$o = x\\beta^T$について\n",
    "$$\n",
    "o_{c=k,n=j} = \\sum_{i=1}^Nx_{ki}\\beta_{ji}\n",
    "$$\n",
    "この式はチャネル$k$において位置$j$について位置$i=1,2,\\cdots N$の全ての影響を総和することを意味している．\n",
    "$x_{ki}$ はチャネル$k$の位置$i$の特徴量を表していて，$\\beta_{ji}$は位置$j$と位置$i$の関係性を表して，この$\\sum$はそれらの積の総和を求めている．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　最初のサイズ変形 (B,C,W,H)  to (B,C,N)\n",
    "X = torch.randn(3,32,64,64)\n",
    "X = X.view(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "# 掛け算\n",
    "X_T = X.permute(0,2,1)\n",
    "S = torch.bmm(X_T, X)\n",
    "\n",
    "# 規格化\n",
    "m = nn.Softmax(dim=-2)\n",
    "attention_map_T = m(S)\n",
    "attention_map = attention_map_T.permute(0,2,1)\n",
    "\n",
    "# Self-Attention Map\n",
    "o = torch.bmm(X, attention_map.permute(0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1×1 Convolutions (pointwise convolution)\n",
    "Self-Attention の制限は非常に強いのでこのままでは学習はうまくいかないことが多い．\n",
    "そこでpointwise convolutionと呼ばれる前処理をした特徴量マップをSelf-Attentionに渡してあげることによって学習がうまくいくようにする．\n",
    "またこれをすることによって特徴量マップのチャネル数を変えることができる(一般にはチャネル数を減らす)ので計算量を調整することができる．\n",
    "\n",
    "またpointwise convolutionでは次の言葉が使われる\n",
    "- query：元の入力$x$の転置に対応するもの\n",
    "- key：元の入力$x$に対応するもの\n",
    "- value：Attention Mapと掛け算する対象\n",
    "\n",
    "出力のチャネルはカーネルの数となる．カーネルのチャネルは入力のチャネルに合わせる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(3,32,64,64)\n",
    "\n",
    "# 1×1の畳み込み層によるpointwise convolutionを用意　\n",
    "query_conv = nn.Conv2d(\n",
    "    in_channels=X.shape[1], out_channels=X.shape[1]//8, kernel_size=1)\n",
    "\n",
    "key_conv = nn.Conv2d(\n",
    "    in_channels=X.shape[1], out_channels=X.shape[1]//8, kernel_size=1)\n",
    "\n",
    "value_conv = nn.Conv2d(\n",
    "    in_channels=X.shape[1], out_channels=X.shape[1], kernel_size=1)\n",
    "\n",
    "# 畳み込みをしてからサイズを変更する\n",
    "proj_query = query_conv(X).view(\n",
    "    X.shape[0], -1, X.shape[2]*X.shape[3])\n",
    "proj_query = proj_query.permute(0,2,1)\n",
    "proj_key = key_conv(X).view(\n",
    "    X.shape[0], -1, X.shape[2]*X.shape[3])\n",
    "\n",
    "# 掛け算\n",
    "S = torch.bmm(proj_query, proj_key) # bmmはバッチごとの掛け算を行う\n",
    "\n",
    "# 規格化\n",
    "m = nn.Softmax(dim=-2)\n",
    "attention_map_T = m(S)\n",
    "attention_map = attention_map_T.permute(0,2,1)\n",
    "\n",
    "# Self-Attention Map\n",
    "proj_value = value_conv(X).view(\n",
    "    X.shape[0], -1, X.shape[2]*X.shape[3])\n",
    "o = torch.bmm(proj_value, attention_map.permute(0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Normalization\n",
    "- Spectral Normalization：畳み込みの重みに対する規格化\n",
    "- Batch Normalization：ディープラーニングモデルを流れるデータに対する規格化\n",
    "\n",
    "***リプシッツ連続性(Lipschitz continuity)：***判別機Dの出力が入力データに対して頑健性を持つこと\n",
    "具体的には層を表す行列の固有値のうち最大の固有値で全ての重みを割ることで規格化している．固有値は固有ベクトルの拡大率なので，一番拡大される固有値で全てを規格化することによって流れるデータが想定外に大きくなることを防ぐ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvTranspose2d(20, 512, kernel_size=(4, 4), stride=(1, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_dim = 20\n",
    "image_size = 64\n",
    "nn.utils.spectral_norm(nn.ConvTranspose2d(\n",
    "    z_dim, image_size*8, kernel_size=4,stride=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-4 Self-Attention GANの学習，生成の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attentionモジュールの実装\n",
    "$$\n",
    "y = x + \\gamma o\n",
    "$$\n",
    "$\\gamma$ は初期値０から始めるが，学習させる対象なのでこれも```nn.Parameter()```を用いて実装する．\n",
    "\n",
    "Attention MapはあとでAttentionの強さを可視化するために出力させてる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(nn.Module):\n",
    "    \"\"\" Self-AttentionのLayer\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim):\n",
    "        super(Self_Attention, self).__init__()\n",
    "        \n",
    "        # 1×1の畳み込み層によるpointwise convolutionを用意　\n",
    "        self.query_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "\n",
    "        self.key_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "\n",
    "        self.value_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        \n",
    "        # 規格化\n",
    "        self.softmax = nn.Softmax(dim=-2)\n",
    "        \n",
    "        # gamma\n",
    "        # 学習させるのでnn.Parameter　を用いる\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 入力変数\n",
    "        X = x\n",
    "        \n",
    "        # 畳み込みをしてからサイズを変更する\n",
    "        proj_query = self.query_conv(X).view(X.shape[0], -1, X.shape[2]*X.shape[3])\n",
    "        proj_query = proj_query.permute(0,2,1)\n",
    "        proj_key = self.key_conv(X).view(X.shape[0], -1, X.shape[2]*X.shape[3])\n",
    "        \n",
    "        \n",
    "        # 掛け算\n",
    "        S = torch.bmm(proj_query, proj_key) # bmmはバッチごとの掛け算を行う  \n",
    "        \n",
    "        # 規格化\n",
    "        attention_map_T = self.softmax(S)\n",
    "        attention_map = attention_map_T.permute(0,2,1)\n",
    "        \n",
    "        # Self-Attention Map\n",
    "        proj_value = self.value_conv(X).view(X.shape[0], -1, X.shape[2]*X.shape[3])\n",
    "        o = torch.bmm(proj_value, attention_map.permute(0,2,1))\n",
    "        \n",
    "        # Self-Attention　MapのテンソルサイズをXに揃えて出力\n",
    "        o = o.view(X.shape[0],X.shape[1],X.shape[2],X.shape[3])\n",
    "        out = x + self.gamma*o\n",
    "        \n",
    "        return out , attention_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generatorの実装\n",
    "DCGANとの変更点は以下の2点である．\n",
    "1. 最後のlaseのLayer以外には転置畳み込み層にSpectral Normalizationを追加する．\n",
    "2. layer3-layer4と，layer4-lastの二箇所にSelf-Attentionモジュールを追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_dim = 20, image_size=64):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            #　ここでConvTranspose2dにspectral_norm を作用させる\n",
    "#             nn.ConvTranspose2d(z_dim, image_size*8, kernel_size=4, stride=1),\n",
    "            nn.utils.spectral_norm(nn.ConvTranspose2d(z_dim, image_size*8, kernel_size=4,stride=1)),\n",
    "            nn.BatchNorm2d(image_size*8),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.ConvTranspose2d(image_size*8, image_size*4, kernel_size=4,stride=2, padding=1)), #spectral_norm\n",
    "            nn.BatchNorm2d(image_size*4),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.ConvTranspose2d(image_size*4, image_size*2, kernel_size=4,stride=2, padding=1)), #spectral_norm\n",
    "            nn.BatchNorm2d(image_size*2),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Self-Attention 層\n",
    "        self.self_attention1 = Self_Attention(in_dim=image_size*2)\n",
    "              \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.ConvTranspose2d(image_size*2, image_size, kernel_size=4,stride=2, padding=1)), #spectral_norm\n",
    "            nn.BatchNorm2d(image_size),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "        # Self-Attention 層\n",
    "        self.self_attention2 = Self_Attention(in_dim=image_size)\n",
    "        \n",
    "        self.last = nn.Sequential(\n",
    "            nn.ConvTranspose2d(image_size, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh())\n",
    "        # 白黒なので出力は1チャネル\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.layer1(z)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out, attention_map1 = self.self_attention1(out)\n",
    "        out = self.layer4(out)\n",
    "        out, attention_map2 = self.self_attention2(out)\n",
    "        out = self.last(out)\n",
    "        \n",
    "        return out, attention_map1, attention_map2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminatorの実装\n",
    "DCGANからの変更点は以下の2点\n",
    "1. last以外のlayerに畳み込み層にSpectral Normalizationを追加する\n",
    "2. layer3-layer4と，layer4-lastの二箇所にSelf-Attentionモジュールを追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_dim=20, image_size=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(1, image_size, kernel_size=4, stride=2, padding=1)), # Spectral Normalizationの追加\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(image_size, image_size*2, kernel_size=4, stride=2, padding=1)),# Spectral Normalizationの追加\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(image_size*2, image_size*4, kernel_size=4, stride=2, padding=1)),# Spectral Normalizationの追加\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "        \n",
    "        # Self-Attention層の追加\n",
    "        self.self_attention1 = Self_Attention(in_dim=image_size*4)\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(image_size*4, image_size*8, kernel_size=4, stride=2, padding=1)),# Spectral Normalizationの追加\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "\n",
    "        # Self-Attention層の追加\n",
    "        self.self_attention2 = Self_Attention(in_dim=image_size*8)\n",
    "        \n",
    "        self.last = nn.Conv2d(image_size*8, 1, kernel_size=4, stride=1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out, attention_map1 = self.self_attention1(out)\n",
    "        out = self.layer4(out)\n",
    "        out, attention_map2 = self.self_attention2(out)\n",
    "        out = self.last(out)\n",
    "        \n",
    "        return out, attention_map1, attention_map2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaderの実装\n",
    "DCGANと同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datapath_list():\n",
    "    \n",
    "    train_img_list = list()\n",
    "    \n",
    "    for img_idx in range(200):\n",
    "        img_path = \"./data/img_78/img_7_\" + str(img_idx) + \".jpg\"\n",
    "        train_img_list.append(img_path)\n",
    "        \n",
    "        img_path = \"./data/img_78/img_8_\" + str(img_idx) + \".jpg\"\n",
    "        train_img_list.append(img_path)\n",
    "        \n",
    "    return train_img_list\n",
    "\n",
    "\n",
    "class ImageTransform():\n",
    "    \"\"\"\n",
    "    画像クラスの前処理\n",
    "    \"\"\"\n",
    "    def __init__(self, mean, std):\n",
    "        self.data_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        return self.data_transform(img)\n",
    "    \n",
    "\n",
    "class GAN_Img_Dataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    画像のDataset\n",
    "    \"\"\"\n",
    "    def __init__(self, file_list, transform):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\"画像の枚数を返す\"\"\"\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \"\"\"前処理をした画像のTensor形式のデータを取得\"\"\"\n",
    "        \n",
    "        img_path = self.file_list[index]\n",
    "        img = Image.open(img_path) #H,W,C\n",
    "        \n",
    "        # 前処理\n",
    "        img_transformed = self.transform(img)\n",
    "        \n",
    "        return img_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderの作成\n",
    "train_img_list = make_datapath_list()\n",
    "# print(train_img_list)\n",
    "\n",
    "# Datasetの作成\n",
    "mean = (0.5,)\n",
    "std = (0.5,)\n",
    "train_dataset = GAN_Img_Dataset(train_img_list, ImageTransform(mean, std))\n",
    "\n",
    "# DataLoaderの作成\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                              batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ネットワークの初期化と学習の実施\n",
    "SAGANの損失関数はhinge version of the adversarial lossと呼ばれるものに変更している\n",
    "$$\n",
    "-\\frac{1}{M}\\sum_{i=1}^M[l_i \\ast \\min (0, -1+y_i) + (1-l_i)\\ast \\min(0, -1-y_i)]\n",
    "$$\n",
    "これは実装では\n",
    "```d_loss_real = torch.nn.ReLU()(1.0-d_out_real).mean()``` となる．ここでReLUは活性化関数ではなくmin(正確にはmax)の部分に相当する．\n",
    "\n",
    "**シグモイドにはしてないがこれは？**\n",
    "- これは```d_out_real```が~~正の値しかとらないのでReLU関数の出力は結果として[0,1]におさまる．~~よってシグモイドを作用させたのと同じ効果がある．\n",
    "\n",
    "またGの方では以下のように損失関数を改める．\n",
    "$$\n",
    "-\\frac{1}{M}\\sum_{i=1}^M D(G(z_i))\n",
    "$$\n",
    "これによって実装は\n",
    "```g_loss = - d_out_fake.mean()``` となる．\n",
    "\n",
    "このような損失関数が用いられているのは経験的にうまくいくからである．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数の作成\n",
    "def train_model(G,D,dataloader, num_epochs):\n",
    "    \n",
    "    # GPU環境の確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス : \", device)\n",
    "    \n",
    "    # 最適化手法の設定\n",
    "    g_lr, d_lr = 0.00001, 0.00004\n",
    "    beta1, beta2 = 0.0, 0.9\n",
    "    g_optimizer = torch.optim.Adam(G.parameters(), g_lr, [beta1, beta2] )\n",
    "    d_optimizer = torch.optim.Adam(D.parameters(), g_lr, [beta1, beta2] )\n",
    "    \n",
    "    \n",
    "    # 誤差関数の定義 \n",
    "#     criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "    \"\"\"\n",
    "    誤差関数はhinge version of the adversarial loss に変更\n",
    "    \"\"\"\n",
    "\n",
    "    # パラメータ\n",
    "    z_dim = 20\n",
    "    mini_batch_size = 64\n",
    "    \n",
    "    # GPU\n",
    "    G.to(device)\n",
    "    D.to(device)\n",
    "    \n",
    "    G.train()\n",
    "    D.train()\n",
    "    \n",
    "    # 高速化\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # 画像枚数\n",
    "    num_train_imgs = len(dataloader.dataset)\n",
    "    batch_size = dataloader.batch_size\n",
    "    \n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    logs = []\n",
    "\n",
    "    \n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        epoch_g_loss = 0.0\n",
    "        epoch_d_loss = 0.0\n",
    "        \n",
    "        print(\"-----------------------------------\")\n",
    "        print(\"Epoch {}/{} \".format(epoch, num_epochs))\n",
    "        print(\"-----------------------------------\")\n",
    "        print(\"---  (train)  ---\")\n",
    "        \n",
    "        # データローダからmini_batchずつ取り出す\n",
    "        for imges in dataloader:\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "            1.　Disicriminatorの学習\n",
    "            \n",
    "            \"\"\"\n",
    "            # ミニバッチがサイズが1だと，バッチノーマライゼーションでエラーになるので避ける\n",
    "            if imges.size()[0] == 1:\n",
    "                continue\n",
    "                \n",
    "            # GPUが使えるなら使う\n",
    "            imges = imges.to(device)\n",
    "            \n",
    "            # 　正解ラベルと偽ラベルを作成\n",
    "            # 最後のepochだけはミニバッチの余りなので数が少なくなる\n",
    "            mini_batch_size = imges.size()[0]\n",
    "#             label_real = torch.full((mini_batch_size,),1).to(device)\n",
    "#             label_fake = torch.full((mini_batch_size,),0).to(device)\n",
    "            \n",
    "            # 真の画像を判定\n",
    "            d_out_real, _, _  = D(imges)   # 出力が増えてるので変更\n",
    "\n",
    "            # 偽の画像を生成して判定\n",
    "            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n",
    "            input_z = input_z.view(input_z.size(0),input_z.size(1), 1,1 ) \n",
    "            fake_images,_ , _ = G(input_z) \n",
    "            d_out_fake,_ ,_  = D(fake_images)\n",
    "            \n",
    "            \"\"\"---------------------------------------------------------------------\n",
    "            この部分を誤差関数の変更に伴って大きく変更\n",
    "            \"\"\"\n",
    "            # 誤差の計算\n",
    "#             d_loss_real = criterion(d_out_real.view(-1), label_real)\n",
    "#             d_loss_fake = criterion(d_out_fake.view(-1), label_fake)\n",
    "#             d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "            d_loss_real = torch.nn.ReLU()(1.0 - d_out_real).mean()\n",
    "            # 誤差関数の出力が0以上になるように調整している．\n",
    "            print(\"d_out_real : \", d_out_real)\n",
    "            print(\"d_loss_real : \",d_loss_real)\n",
    "            \n",
    "            d_loss_fake = torch.nn.ReLU()(1.0+d_out_fake).mean()\n",
    "            # これは[0,1]からは出てしまうが，d_out_fakeの出力が0に近いほど良いというところを実装している\n",
    "            \n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            \n",
    "\n",
    "            \"\"\"---------------------------------------------------------------------\"\"\"\n",
    "            \n",
    "            # backward()\n",
    "            g_optimizer.zero_grad()  # ここでどちらも初期化すること！\n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "        \n",
    "            \"\"\"\n",
    "            \n",
    "            2.　Generatorの学習\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "            # 偽画像を生成して判定\n",
    "            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n",
    "            input_z = input_z.view(input_z.size(0),input_z.size(1), 1,1 )\n",
    "            fake_images,_ ,_  = G(input_z)\n",
    "            d_out_fake,_ ,_ = D(fake_images)\n",
    "            \n",
    "        \n",
    "            \"\"\"---------------------------------------------------------------------\n",
    "            この部分を誤差関数の変更に伴って大きく変更\n",
    "            \"\"\"\n",
    "            # 誤差の計算\n",
    "#             g_loss = criterion(d_out_fake.view(-1), label_real)\n",
    "\n",
    "            g_loss = - d_out_fake.mean()\n",
    "\n",
    "\n",
    "            \"\"\"---------------------------------------------------------------------\"\"\"\n",
    "            \n",
    "            # backward()\n",
    "            g_optimizer.zero_grad()  # ここでどちらも初期化すること！\n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            \"\"\"\n",
    "            \n",
    "            3.　記録\n",
    "            \n",
    "            \"\"\"\n",
    "            epoch_d_loss += d_loss.item()\n",
    "            epoch_g_loss += g_loss.item()\n",
    "            iteration += 1\n",
    "            \n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print(\"----------------------------\")\n",
    "        print('epoch {} || Epoch_D_Loss:{:.4f} ||Epoch_G_Loss:{:.4f}'.format(\n",
    "            epoch, epoch_d_loss/batch_size, epoch_g_loss/batch_size))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "        if ((epoch+1)%20==0):\n",
    "            torch.save(G.state_dict(), \"weights/sagan_g_\"+str(epoch+1) + \".pth\")\n",
    "            torch.save(D.state_dict(), \"weights/sagan_d_\"+str(epoch+1) + \".pth\")\n",
    "\n",
    "    return G, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス :  cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"使用デバイス : \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nextットワークの初期化完了\n"
     ]
    }
   ],
   "source": [
    "# ネットワークの初期化\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        # Conv2dとConvTranspose2dの初期化\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02) # mean  = 0.0 , std = 0.02\n",
    "        nn.init.constant_(m.bias.data,0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        #BatchNorm2dの初期化\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data,0)\n",
    "        \n",
    "G = Generator(z_dim=20, image_size=64)\n",
    "D = Discriminator(z_dim=20, image_size=64)\n",
    "        \n",
    "# 初期化の実施\n",
    "G.apply(weights_init)\n",
    "D.apply(weights_init)\n",
    "\n",
    "print(\"nextットワークの初期化完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス :  cpu\n",
      "-----------------------------------\n",
      "Epoch 0/2 \n",
      "-----------------------------------\n",
      "---  (train)  ---\n",
      "d_out_real :  tensor([[[[-0.0831]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0535]]],\n",
      "\n",
      "\n",
      "        [[[-0.0240]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0177]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0438]]],\n",
      "\n",
      "\n",
      "        [[[-0.0400]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1094]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0449]]],\n",
      "\n",
      "\n",
      "        [[[-0.0414]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0598]]],\n",
      "\n",
      "\n",
      "        [[[-0.0028]]],\n",
      "\n",
      "\n",
      "        [[[-0.0711]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0735]]],\n",
      "\n",
      "\n",
      "        [[[-0.0375]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0290]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0026]]],\n",
      "\n",
      "\n",
      "        [[[-0.0688]]],\n",
      "\n",
      "\n",
      "        [[[-0.0287]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0765]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0228]]],\n",
      "\n",
      "\n",
      "        [[[-0.0720]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0173]]],\n",
      "\n",
      "\n",
      "        [[[-0.0326]]],\n",
      "\n",
      "\n",
      "        [[[-0.0062]]],\n",
      "\n",
      "\n",
      "        [[[-0.0268]]],\n",
      "\n",
      "\n",
      "        [[[-0.0506]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0445]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0526]]],\n",
      "\n",
      "\n",
      "        [[[-0.0278]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0267]]],\n",
      "\n",
      "\n",
      "        [[[-0.0364]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0309]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0111]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0313]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0414]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1094]]],\n",
      "\n",
      "\n",
      "        [[[-0.0289]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0547]]],\n",
      "\n",
      "\n",
      "        [[[-0.0487]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0620]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0579]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0662]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0604]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1544]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0052]]],\n",
      "\n",
      "\n",
      "        [[[-0.0222]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0355]]],\n",
      "\n",
      "\n",
      "        [[[-0.0095]]],\n",
      "\n",
      "\n",
      "        [[[-0.0015]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0495]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0702]]],\n",
      "\n",
      "\n",
      "        [[[-0.0084]]],\n",
      "\n",
      "\n",
      "        [[[-0.0613]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0646]]],\n",
      "\n",
      "\n",
      "        [[[-0.0093]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0803]]],\n",
      "\n",
      "\n",
      "        [[[-0.0758]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0706]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0044]]],\n",
      "\n",
      "\n",
      "        [[[-0.0531]]],\n",
      "\n",
      "\n",
      "        [[[-0.0734]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0316]]],\n",
      "\n",
      "\n",
      "        [[[-0.0589]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0288]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "d_loss_real :  tensor(0.9892, grad_fn=<MeanBackward0>)\n",
      "d_out_real :  tensor([[[[ 0.0608]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0402]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0400]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0289]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0193]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0774]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0356]]],\n",
      "\n",
      "\n",
      "        [[[-0.0192]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0516]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0905]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0065]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0065]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0324]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0491]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0346]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0452]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0405]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0522]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0763]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0740]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0766]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0052]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0063]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0496]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0466]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0590]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0919]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0519]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0346]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0666]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0486]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0432]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0770]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0508]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0584]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0941]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0504]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0642]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0355]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0461]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0395]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0613]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0423]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0944]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0420]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0780]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0719]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0902]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0105]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0829]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0343]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0209]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0121]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0443]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0199]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0576]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0505]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0762]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0358]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0454]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0511]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0710]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0498]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0435]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "d_loss_real :  tensor(0.9512, grad_fn=<MeanBackward0>)\n",
      "d_out_real :  tensor([[[[0.0628]]],\n",
      "\n",
      "\n",
      "        [[[0.0541]]],\n",
      "\n",
      "\n",
      "        [[[0.0346]]],\n",
      "\n",
      "\n",
      "        [[[0.0555]]],\n",
      "\n",
      "\n",
      "        [[[0.0688]]],\n",
      "\n",
      "\n",
      "        [[[0.0797]]],\n",
      "\n",
      "\n",
      "        [[[0.0695]]],\n",
      "\n",
      "\n",
      "        [[[0.0453]]],\n",
      "\n",
      "\n",
      "        [[[0.0520]]],\n",
      "\n",
      "\n",
      "        [[[0.0625]]],\n",
      "\n",
      "\n",
      "        [[[0.0470]]],\n",
      "\n",
      "\n",
      "        [[[0.0544]]],\n",
      "\n",
      "\n",
      "        [[[0.0684]]],\n",
      "\n",
      "\n",
      "        [[[0.0286]]],\n",
      "\n",
      "\n",
      "        [[[0.0712]]],\n",
      "\n",
      "\n",
      "        [[[0.0466]]],\n",
      "\n",
      "\n",
      "        [[[0.0768]]],\n",
      "\n",
      "\n",
      "        [[[0.0499]]],\n",
      "\n",
      "\n",
      "        [[[0.0573]]],\n",
      "\n",
      "\n",
      "        [[[0.0770]]],\n",
      "\n",
      "\n",
      "        [[[0.0307]]],\n",
      "\n",
      "\n",
      "        [[[0.0315]]],\n",
      "\n",
      "\n",
      "        [[[0.0418]]],\n",
      "\n",
      "\n",
      "        [[[0.0608]]],\n",
      "\n",
      "\n",
      "        [[[0.0673]]],\n",
      "\n",
      "\n",
      "        [[[0.0658]]],\n",
      "\n",
      "\n",
      "        [[[0.0202]]],\n",
      "\n",
      "\n",
      "        [[[0.0905]]],\n",
      "\n",
      "\n",
      "        [[[0.0732]]],\n",
      "\n",
      "\n",
      "        [[[0.0260]]],\n",
      "\n",
      "\n",
      "        [[[0.0516]]],\n",
      "\n",
      "\n",
      "        [[[0.0364]]],\n",
      "\n",
      "\n",
      "        [[[0.1103]]],\n",
      "\n",
      "\n",
      "        [[[0.0702]]],\n",
      "\n",
      "\n",
      "        [[[0.0623]]],\n",
      "\n",
      "\n",
      "        [[[0.0533]]],\n",
      "\n",
      "\n",
      "        [[[0.0866]]],\n",
      "\n",
      "\n",
      "        [[[0.0313]]],\n",
      "\n",
      "\n",
      "        [[[0.0359]]],\n",
      "\n",
      "\n",
      "        [[[0.0806]]],\n",
      "\n",
      "\n",
      "        [[[0.0342]]],\n",
      "\n",
      "\n",
      "        [[[0.0367]]],\n",
      "\n",
      "\n",
      "        [[[0.0627]]],\n",
      "\n",
      "\n",
      "        [[[0.0421]]],\n",
      "\n",
      "\n",
      "        [[[0.0409]]],\n",
      "\n",
      "\n",
      "        [[[0.0971]]],\n",
      "\n",
      "\n",
      "        [[[0.0425]]],\n",
      "\n",
      "\n",
      "        [[[0.0718]]],\n",
      "\n",
      "\n",
      "        [[[0.0398]]],\n",
      "\n",
      "\n",
      "        [[[0.0780]]],\n",
      "\n",
      "\n",
      "        [[[0.0197]]],\n",
      "\n",
      "\n",
      "        [[[0.0315]]],\n",
      "\n",
      "\n",
      "        [[[0.0248]]],\n",
      "\n",
      "\n",
      "        [[[0.0547]]],\n",
      "\n",
      "\n",
      "        [[[0.0575]]],\n",
      "\n",
      "\n",
      "        [[[0.0487]]],\n",
      "\n",
      "\n",
      "        [[[0.0238]]],\n",
      "\n",
      "\n",
      "        [[[0.0485]]],\n",
      "\n",
      "\n",
      "        [[[0.0607]]],\n",
      "\n",
      "\n",
      "        [[[0.0504]]],\n",
      "\n",
      "\n",
      "        [[[0.0701]]],\n",
      "\n",
      "\n",
      "        [[[0.0706]]],\n",
      "\n",
      "\n",
      "        [[[0.0877]]],\n",
      "\n",
      "\n",
      "        [[[0.0743]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "d_loss_real :  tensor(0.9444, grad_fn=<MeanBackward0>)\n",
      "d_out_real :  tensor([[[[0.1368]]],\n",
      "\n",
      "\n",
      "        [[[0.0676]]],\n",
      "\n",
      "\n",
      "        [[[0.0763]]],\n",
      "\n",
      "\n",
      "        [[[0.0826]]],\n",
      "\n",
      "\n",
      "        [[[0.0634]]],\n",
      "\n",
      "\n",
      "        [[[0.0594]]],\n",
      "\n",
      "\n",
      "        [[[0.0875]]],\n",
      "\n",
      "\n",
      "        [[[0.0705]]],\n",
      "\n",
      "\n",
      "        [[[0.1209]]],\n",
      "\n",
      "\n",
      "        [[[0.1017]]],\n",
      "\n",
      "\n",
      "        [[[0.0666]]],\n",
      "\n",
      "\n",
      "        [[[0.0893]]],\n",
      "\n",
      "\n",
      "        [[[0.0677]]],\n",
      "\n",
      "\n",
      "        [[[0.0928]]],\n",
      "\n",
      "\n",
      "        [[[0.0488]]],\n",
      "\n",
      "\n",
      "        [[[0.0819]]],\n",
      "\n",
      "\n",
      "        [[[0.0611]]],\n",
      "\n",
      "\n",
      "        [[[0.0620]]],\n",
      "\n",
      "\n",
      "        [[[0.0455]]],\n",
      "\n",
      "\n",
      "        [[[0.0467]]],\n",
      "\n",
      "\n",
      "        [[[0.1040]]],\n",
      "\n",
      "\n",
      "        [[[0.0735]]],\n",
      "\n",
      "\n",
      "        [[[0.0751]]],\n",
      "\n",
      "\n",
      "        [[[0.0984]]],\n",
      "\n",
      "\n",
      "        [[[0.0669]]],\n",
      "\n",
      "\n",
      "        [[[0.0458]]],\n",
      "\n",
      "\n",
      "        [[[0.0811]]],\n",
      "\n",
      "\n",
      "        [[[0.0722]]],\n",
      "\n",
      "\n",
      "        [[[0.0914]]],\n",
      "\n",
      "\n",
      "        [[[0.0815]]],\n",
      "\n",
      "\n",
      "        [[[0.0607]]],\n",
      "\n",
      "\n",
      "        [[[0.0655]]],\n",
      "\n",
      "\n",
      "        [[[0.1201]]],\n",
      "\n",
      "\n",
      "        [[[0.0311]]],\n",
      "\n",
      "\n",
      "        [[[0.0674]]],\n",
      "\n",
      "\n",
      "        [[[0.0774]]],\n",
      "\n",
      "\n",
      "        [[[0.0752]]],\n",
      "\n",
      "\n",
      "        [[[0.0836]]],\n",
      "\n",
      "\n",
      "        [[[0.0713]]],\n",
      "\n",
      "\n",
      "        [[[0.0470]]],\n",
      "\n",
      "\n",
      "        [[[0.0715]]],\n",
      "\n",
      "\n",
      "        [[[0.0678]]],\n",
      "\n",
      "\n",
      "        [[[0.0660]]],\n",
      "\n",
      "\n",
      "        [[[0.0508]]],\n",
      "\n",
      "\n",
      "        [[[0.0725]]],\n",
      "\n",
      "\n",
      "        [[[0.0559]]],\n",
      "\n",
      "\n",
      "        [[[0.0846]]],\n",
      "\n",
      "\n",
      "        [[[0.0542]]],\n",
      "\n",
      "\n",
      "        [[[0.0956]]],\n",
      "\n",
      "\n",
      "        [[[0.0950]]],\n",
      "\n",
      "\n",
      "        [[[0.0887]]],\n",
      "\n",
      "\n",
      "        [[[0.0590]]],\n",
      "\n",
      "\n",
      "        [[[0.1015]]],\n",
      "\n",
      "\n",
      "        [[[0.0875]]],\n",
      "\n",
      "\n",
      "        [[[0.0868]]],\n",
      "\n",
      "\n",
      "        [[[0.0403]]],\n",
      "\n",
      "\n",
      "        [[[0.0695]]],\n",
      "\n",
      "\n",
      "        [[[0.0978]]],\n",
      "\n",
      "\n",
      "        [[[0.0415]]],\n",
      "\n",
      "\n",
      "        [[[0.0712]]],\n",
      "\n",
      "\n",
      "        [[[0.0854]]],\n",
      "\n",
      "\n",
      "        [[[0.0437]]],\n",
      "\n",
      "\n",
      "        [[[0.0903]]],\n",
      "\n",
      "\n",
      "        [[[0.1032]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "d_loss_real :  tensor(0.9250, grad_fn=<MeanBackward0>)\n",
      "d_out_real :  tensor([[[[0.0825]]],\n",
      "\n",
      "\n",
      "        [[[0.0932]]],\n",
      "\n",
      "\n",
      "        [[[0.0901]]],\n",
      "\n",
      "\n",
      "        [[[0.1030]]],\n",
      "\n",
      "\n",
      "        [[[0.0990]]],\n",
      "\n",
      "\n",
      "        [[[0.1052]]],\n",
      "\n",
      "\n",
      "        [[[0.1047]]],\n",
      "\n",
      "\n",
      "        [[[0.0757]]],\n",
      "\n",
      "\n",
      "        [[[0.0969]]],\n",
      "\n",
      "\n",
      "        [[[0.1118]]],\n",
      "\n",
      "\n",
      "        [[[0.0735]]],\n",
      "\n",
      "\n",
      "        [[[0.0693]]],\n",
      "\n",
      "\n",
      "        [[[0.0725]]],\n",
      "\n",
      "\n",
      "        [[[0.0954]]],\n",
      "\n",
      "\n",
      "        [[[0.1076]]],\n",
      "\n",
      "\n",
      "        [[[0.0827]]],\n",
      "\n",
      "\n",
      "        [[[0.1009]]],\n",
      "\n",
      "\n",
      "        [[[0.0913]]],\n",
      "\n",
      "\n",
      "        [[[0.0469]]],\n",
      "\n",
      "\n",
      "        [[[0.0840]]],\n",
      "\n",
      "\n",
      "        [[[0.0812]]],\n",
      "\n",
      "\n",
      "        [[[0.0962]]],\n",
      "\n",
      "\n",
      "        [[[0.1231]]],\n",
      "\n",
      "\n",
      "        [[[0.1224]]],\n",
      "\n",
      "\n",
      "        [[[0.0746]]],\n",
      "\n",
      "\n",
      "        [[[0.0801]]],\n",
      "\n",
      "\n",
      "        [[[0.0980]]],\n",
      "\n",
      "\n",
      "        [[[0.0932]]],\n",
      "\n",
      "\n",
      "        [[[0.1082]]],\n",
      "\n",
      "\n",
      "        [[[0.1080]]],\n",
      "\n",
      "\n",
      "        [[[0.0942]]],\n",
      "\n",
      "\n",
      "        [[[0.0648]]],\n",
      "\n",
      "\n",
      "        [[[0.1433]]],\n",
      "\n",
      "\n",
      "        [[[0.1115]]],\n",
      "\n",
      "\n",
      "        [[[0.0578]]],\n",
      "\n",
      "\n",
      "        [[[0.0778]]],\n",
      "\n",
      "\n",
      "        [[[0.1161]]],\n",
      "\n",
      "\n",
      "        [[[0.0976]]],\n",
      "\n",
      "\n",
      "        [[[0.0983]]],\n",
      "\n",
      "\n",
      "        [[[0.1240]]],\n",
      "\n",
      "\n",
      "        [[[0.1233]]],\n",
      "\n",
      "\n",
      "        [[[0.0827]]],\n",
      "\n",
      "\n",
      "        [[[0.1040]]],\n",
      "\n",
      "\n",
      "        [[[0.1078]]],\n",
      "\n",
      "\n",
      "        [[[0.0778]]],\n",
      "\n",
      "\n",
      "        [[[0.0670]]],\n",
      "\n",
      "\n",
      "        [[[0.0850]]],\n",
      "\n",
      "\n",
      "        [[[0.1421]]],\n",
      "\n",
      "\n",
      "        [[[0.0880]]],\n",
      "\n",
      "\n",
      "        [[[0.0640]]],\n",
      "\n",
      "\n",
      "        [[[0.0587]]],\n",
      "\n",
      "\n",
      "        [[[0.0677]]],\n",
      "\n",
      "\n",
      "        [[[0.0733]]],\n",
      "\n",
      "\n",
      "        [[[0.1111]]],\n",
      "\n",
      "\n",
      "        [[[0.1113]]],\n",
      "\n",
      "\n",
      "        [[[0.0795]]],\n",
      "\n",
      "\n",
      "        [[[0.0723]]],\n",
      "\n",
      "\n",
      "        [[[0.1108]]],\n",
      "\n",
      "\n",
      "        [[[0.0933]]],\n",
      "\n",
      "\n",
      "        [[[0.1119]]],\n",
      "\n",
      "\n",
      "        [[[0.1064]]],\n",
      "\n",
      "\n",
      "        [[[0.0736]]],\n",
      "\n",
      "\n",
      "        [[[0.0804]]],\n",
      "\n",
      "\n",
      "        [[[0.0917]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "d_loss_real :  tensor(0.9072, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_out_real :  tensor([[[[0.0887]]],\n",
      "\n",
      "\n",
      "        [[[0.1356]]],\n",
      "\n",
      "\n",
      "        [[[0.0805]]],\n",
      "\n",
      "\n",
      "        [[[0.0859]]],\n",
      "\n",
      "\n",
      "        [[[0.0789]]],\n",
      "\n",
      "\n",
      "        [[[0.1087]]],\n",
      "\n",
      "\n",
      "        [[[0.1177]]],\n",
      "\n",
      "\n",
      "        [[[0.1282]]],\n",
      "\n",
      "\n",
      "        [[[0.0869]]],\n",
      "\n",
      "\n",
      "        [[[0.1236]]],\n",
      "\n",
      "\n",
      "        [[[0.1069]]],\n",
      "\n",
      "\n",
      "        [[[0.0880]]],\n",
      "\n",
      "\n",
      "        [[[0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1237]]],\n",
      "\n",
      "\n",
      "        [[[0.1292]]],\n",
      "\n",
      "\n",
      "        [[[0.1193]]],\n",
      "\n",
      "\n",
      "        [[[0.1262]]],\n",
      "\n",
      "\n",
      "        [[[0.1079]]],\n",
      "\n",
      "\n",
      "        [[[0.1157]]],\n",
      "\n",
      "\n",
      "        [[[0.1026]]],\n",
      "\n",
      "\n",
      "        [[[0.0808]]],\n",
      "\n",
      "\n",
      "        [[[0.0957]]],\n",
      "\n",
      "\n",
      "        [[[0.0948]]],\n",
      "\n",
      "\n",
      "        [[[0.1076]]],\n",
      "\n",
      "\n",
      "        [[[0.0783]]],\n",
      "\n",
      "\n",
      "        [[[0.1243]]],\n",
      "\n",
      "\n",
      "        [[[0.0915]]],\n",
      "\n",
      "\n",
      "        [[[0.0985]]],\n",
      "\n",
      "\n",
      "        [[[0.1192]]],\n",
      "\n",
      "\n",
      "        [[[0.1177]]],\n",
      "\n",
      "\n",
      "        [[[0.1209]]],\n",
      "\n",
      "\n",
      "        [[[0.1198]]],\n",
      "\n",
      "\n",
      "        [[[0.1055]]],\n",
      "\n",
      "\n",
      "        [[[0.0912]]],\n",
      "\n",
      "\n",
      "        [[[0.1320]]],\n",
      "\n",
      "\n",
      "        [[[0.1213]]],\n",
      "\n",
      "\n",
      "        [[[0.1041]]],\n",
      "\n",
      "\n",
      "        [[[0.1118]]],\n",
      "\n",
      "\n",
      "        [[[0.1120]]],\n",
      "\n",
      "\n",
      "        [[[0.1262]]],\n",
      "\n",
      "\n",
      "        [[[0.0984]]],\n",
      "\n",
      "\n",
      "        [[[0.0844]]],\n",
      "\n",
      "\n",
      "        [[[0.1128]]],\n",
      "\n",
      "\n",
      "        [[[0.0995]]],\n",
      "\n",
      "\n",
      "        [[[0.0705]]],\n",
      "\n",
      "\n",
      "        [[[0.1121]]],\n",
      "\n",
      "\n",
      "        [[[0.1149]]],\n",
      "\n",
      "\n",
      "        [[[0.0988]]],\n",
      "\n",
      "\n",
      "        [[[0.1234]]],\n",
      "\n",
      "\n",
      "        [[[0.0877]]],\n",
      "\n",
      "\n",
      "        [[[0.1068]]],\n",
      "\n",
      "\n",
      "        [[[0.1008]]],\n",
      "\n",
      "\n",
      "        [[[0.0913]]],\n",
      "\n",
      "\n",
      "        [[[0.1019]]],\n",
      "\n",
      "\n",
      "        [[[0.1308]]],\n",
      "\n",
      "\n",
      "        [[[0.0561]]],\n",
      "\n",
      "\n",
      "        [[[0.1153]]],\n",
      "\n",
      "\n",
      "        [[[0.0767]]],\n",
      "\n",
      "\n",
      "        [[[0.1020]]],\n",
      "\n",
      "\n",
      "        [[[0.0886]]],\n",
      "\n",
      "\n",
      "        [[[0.1304]]],\n",
      "\n",
      "\n",
      "        [[[0.1296]]],\n",
      "\n",
      "\n",
      "        [[[0.0947]]],\n",
      "\n",
      "\n",
      "        [[[0.0885]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "d_loss_real :  tensor(0.8949, grad_fn=<MeanBackward0>)\n",
      "d_out_real :  tensor([[[[0.1377]]],\n",
      "\n",
      "\n",
      "        [[[0.1517]]],\n",
      "\n",
      "\n",
      "        [[[0.1139]]],\n",
      "\n",
      "\n",
      "        [[[0.1113]]],\n",
      "\n",
      "\n",
      "        [[[0.1247]]],\n",
      "\n",
      "\n",
      "        [[[0.1295]]],\n",
      "\n",
      "\n",
      "        [[[0.1287]]],\n",
      "\n",
      "\n",
      "        [[[0.1300]]],\n",
      "\n",
      "\n",
      "        [[[0.1122]]],\n",
      "\n",
      "\n",
      "        [[[0.1345]]],\n",
      "\n",
      "\n",
      "        [[[0.1247]]],\n",
      "\n",
      "\n",
      "        [[[0.1454]]],\n",
      "\n",
      "\n",
      "        [[[0.1142]]],\n",
      "\n",
      "\n",
      "        [[[0.1417]]],\n",
      "\n",
      "\n",
      "        [[[0.1305]]],\n",
      "\n",
      "\n",
      "        [[[0.1509]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "d_loss_real :  tensor(0.8699, grad_fn=<MeanBackward0>)\n",
      "----------------------------\n",
      "epoch 0 || Epoch_D_Loss:0.2122 ||Epoch_G_Loss:-0.0016\n",
      "timer:  157.6793 sec.\n",
      "-----------------------------------\n",
      "Epoch 1/2 \n",
      "-----------------------------------\n",
      "---  (train)  ---\n",
      "d_out_real :  tensor([[[[0.1500]]],\n",
      "\n",
      "\n",
      "        [[[0.1227]]],\n",
      "\n",
      "\n",
      "        [[[0.1489]]],\n",
      "\n",
      "\n",
      "        [[[0.1621]]],\n",
      "\n",
      "\n",
      "        [[[0.1538]]],\n",
      "\n",
      "\n",
      "        [[[0.1407]]],\n",
      "\n",
      "\n",
      "        [[[0.1569]]],\n",
      "\n",
      "\n",
      "        [[[0.1781]]],\n",
      "\n",
      "\n",
      "        [[[0.1321]]],\n",
      "\n",
      "\n",
      "        [[[0.1438]]],\n",
      "\n",
      "\n",
      "        [[[0.1247]]],\n",
      "\n",
      "\n",
      "        [[[0.1720]]],\n",
      "\n",
      "\n",
      "        [[[0.1549]]],\n",
      "\n",
      "\n",
      "        [[[0.1532]]],\n",
      "\n",
      "\n",
      "        [[[0.1639]]],\n",
      "\n",
      "\n",
      "        [[[0.1570]]],\n",
      "\n",
      "\n",
      "        [[[0.1507]]],\n",
      "\n",
      "\n",
      "        [[[0.1544]]],\n",
      "\n",
      "\n",
      "        [[[0.1665]]],\n",
      "\n",
      "\n",
      "        [[[0.1267]]],\n",
      "\n",
      "\n",
      "        [[[0.1614]]],\n",
      "\n",
      "\n",
      "        [[[0.1433]]],\n",
      "\n",
      "\n",
      "        [[[0.1289]]],\n",
      "\n",
      "\n",
      "        [[[0.1862]]],\n",
      "\n",
      "\n",
      "        [[[0.1528]]],\n",
      "\n",
      "\n",
      "        [[[0.1643]]],\n",
      "\n",
      "\n",
      "        [[[0.1638]]],\n",
      "\n",
      "\n",
      "        [[[0.1583]]],\n",
      "\n",
      "\n",
      "        [[[0.1133]]],\n",
      "\n",
      "\n",
      "        [[[0.1380]]],\n",
      "\n",
      "\n",
      "        [[[0.1445]]],\n",
      "\n",
      "\n",
      "        [[[0.1699]]],\n",
      "\n",
      "\n",
      "        [[[0.1682]]],\n",
      "\n",
      "\n",
      "        [[[0.1486]]],\n",
      "\n",
      "\n",
      "        [[[0.1008]]],\n",
      "\n",
      "\n",
      "        [[[0.1504]]],\n",
      "\n",
      "\n",
      "        [[[0.1568]]],\n",
      "\n",
      "\n",
      "        [[[0.1033]]],\n",
      "\n",
      "\n",
      "        [[[0.1495]]],\n",
      "\n",
      "\n",
      "        [[[0.1680]]],\n",
      "\n",
      "\n",
      "        [[[0.1519]]],\n",
      "\n",
      "\n",
      "        [[[0.1258]]],\n",
      "\n",
      "\n",
      "        [[[0.1697]]],\n",
      "\n",
      "\n",
      "        [[[0.1595]]],\n",
      "\n",
      "\n",
      "        [[[0.1282]]],\n",
      "\n",
      "\n",
      "        [[[0.1665]]],\n",
      "\n",
      "\n",
      "        [[[0.1447]]],\n",
      "\n",
      "\n",
      "        [[[0.1425]]],\n",
      "\n",
      "\n",
      "        [[[0.1415]]],\n",
      "\n",
      "\n",
      "        [[[0.1517]]],\n",
      "\n",
      "\n",
      "        [[[0.1344]]],\n",
      "\n",
      "\n",
      "        [[[0.1354]]],\n",
      "\n",
      "\n",
      "        [[[0.1289]]],\n",
      "\n",
      "\n",
      "        [[[0.1557]]],\n",
      "\n",
      "\n",
      "        [[[0.1683]]],\n",
      "\n",
      "\n",
      "        [[[0.1408]]],\n",
      "\n",
      "\n",
      "        [[[0.1885]]],\n",
      "\n",
      "\n",
      "        [[[0.1276]]],\n",
      "\n",
      "\n",
      "        [[[0.1745]]],\n",
      "\n",
      "\n",
      "        [[[0.1313]]],\n",
      "\n",
      "\n",
      "        [[[0.1518]]],\n",
      "\n",
      "\n",
      "        [[[0.1610]]],\n",
      "\n",
      "\n",
      "        [[[0.1659]]],\n",
      "\n",
      "\n",
      "        [[[0.1387]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "d_loss_real :  tensor(0.8505, grad_fn=<MeanBackward0>)\n",
      "d_out_real :  tensor([[[[0.1374]]],\n",
      "\n",
      "\n",
      "        [[[0.1843]]],\n",
      "\n",
      "\n",
      "        [[[0.1512]]],\n",
      "\n",
      "\n",
      "        [[[0.1658]]],\n",
      "\n",
      "\n",
      "        [[[0.1908]]],\n",
      "\n",
      "\n",
      "        [[[0.1554]]],\n",
      "\n",
      "\n",
      "        [[[0.1685]]],\n",
      "\n",
      "\n",
      "        [[[0.1933]]],\n",
      "\n",
      "\n",
      "        [[[0.1957]]],\n",
      "\n",
      "\n",
      "        [[[0.1924]]],\n",
      "\n",
      "\n",
      "        [[[0.1766]]],\n",
      "\n",
      "\n",
      "        [[[0.1979]]],\n",
      "\n",
      "\n",
      "        [[[0.1887]]],\n",
      "\n",
      "\n",
      "        [[[0.1884]]],\n",
      "\n",
      "\n",
      "        [[[0.1611]]],\n",
      "\n",
      "\n",
      "        [[[0.1684]]],\n",
      "\n",
      "\n",
      "        [[[0.1489]]],\n",
      "\n",
      "\n",
      "        [[[0.1941]]],\n",
      "\n",
      "\n",
      "        [[[0.1936]]],\n",
      "\n",
      "\n",
      "        [[[0.1825]]],\n",
      "\n",
      "\n",
      "        [[[0.1397]]],\n",
      "\n",
      "\n",
      "        [[[0.1964]]],\n",
      "\n",
      "\n",
      "        [[[0.1606]]],\n",
      "\n",
      "\n",
      "        [[[0.1752]]],\n",
      "\n",
      "\n",
      "        [[[0.1844]]],\n",
      "\n",
      "\n",
      "        [[[0.1443]]],\n",
      "\n",
      "\n",
      "        [[[0.1823]]],\n",
      "\n",
      "\n",
      "        [[[0.1488]]],\n",
      "\n",
      "\n",
      "        [[[0.1971]]],\n",
      "\n",
      "\n",
      "        [[[0.1253]]],\n",
      "\n",
      "\n",
      "        [[[0.1661]]],\n",
      "\n",
      "\n",
      "        [[[0.1522]]],\n",
      "\n",
      "\n",
      "        [[[0.1781]]],\n",
      "\n",
      "\n",
      "        [[[0.1749]]],\n",
      "\n",
      "\n",
      "        [[[0.1633]]],\n",
      "\n",
      "\n",
      "        [[[0.1717]]],\n",
      "\n",
      "\n",
      "        [[[0.2004]]],\n",
      "\n",
      "\n",
      "        [[[0.2194]]],\n",
      "\n",
      "\n",
      "        [[[0.1368]]],\n",
      "\n",
      "\n",
      "        [[[0.1701]]],\n",
      "\n",
      "\n",
      "        [[[0.1727]]],\n",
      "\n",
      "\n",
      "        [[[0.2007]]],\n",
      "\n",
      "\n",
      "        [[[0.1713]]],\n",
      "\n",
      "\n",
      "        [[[0.1777]]],\n",
      "\n",
      "\n",
      "        [[[0.1371]]],\n",
      "\n",
      "\n",
      "        [[[0.1685]]],\n",
      "\n",
      "\n",
      "        [[[0.1794]]],\n",
      "\n",
      "\n",
      "        [[[0.1602]]],\n",
      "\n",
      "\n",
      "        [[[0.1494]]],\n",
      "\n",
      "\n",
      "        [[[0.1708]]],\n",
      "\n",
      "\n",
      "        [[[0.1789]]],\n",
      "\n",
      "\n",
      "        [[[0.1489]]],\n",
      "\n",
      "\n",
      "        [[[0.1487]]],\n",
      "\n",
      "\n",
      "        [[[0.1645]]],\n",
      "\n",
      "\n",
      "        [[[0.1469]]],\n",
      "\n",
      "\n",
      "        [[[0.1700]]],\n",
      "\n",
      "\n",
      "        [[[0.1585]]],\n",
      "\n",
      "\n",
      "        [[[0.1768]]],\n",
      "\n",
      "\n",
      "        [[[0.1509]]],\n",
      "\n",
      "\n",
      "        [[[0.1488]]],\n",
      "\n",
      "\n",
      "        [[[0.1486]]],\n",
      "\n",
      "\n",
      "        [[[0.1723]]],\n",
      "\n",
      "\n",
      "        [[[0.1486]]],\n",
      "\n",
      "\n",
      "        [[[0.1687]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "d_loss_real :  tensor(0.8306, grad_fn=<MeanBackward0>)\n",
      "d_out_real :  tensor([[[[0.1687]]],\n",
      "\n",
      "\n",
      "        [[[0.2175]]],\n",
      "\n",
      "\n",
      "        [[[0.1623]]],\n",
      "\n",
      "\n",
      "        [[[0.1987]]],\n",
      "\n",
      "\n",
      "        [[[0.1611]]],\n",
      "\n",
      "\n",
      "        [[[0.2007]]],\n",
      "\n",
      "\n",
      "        [[[0.2230]]],\n",
      "\n",
      "\n",
      "        [[[0.1819]]],\n",
      "\n",
      "\n",
      "        [[[0.2102]]],\n",
      "\n",
      "\n",
      "        [[[0.1910]]],\n",
      "\n",
      "\n",
      "        [[[0.1821]]],\n",
      "\n",
      "\n",
      "        [[[0.2101]]],\n",
      "\n",
      "\n",
      "        [[[0.1896]]],\n",
      "\n",
      "\n",
      "        [[[0.1901]]],\n",
      "\n",
      "\n",
      "        [[[0.1815]]],\n",
      "\n",
      "\n",
      "        [[[0.2248]]],\n",
      "\n",
      "\n",
      "        [[[0.1838]]],\n",
      "\n",
      "\n",
      "        [[[0.1776]]],\n",
      "\n",
      "\n",
      "        [[[0.1880]]],\n",
      "\n",
      "\n",
      "        [[[0.2103]]],\n",
      "\n",
      "\n",
      "        [[[0.2290]]],\n",
      "\n",
      "\n",
      "        [[[0.1957]]],\n",
      "\n",
      "\n",
      "        [[[0.1893]]],\n",
      "\n",
      "\n",
      "        [[[0.2143]]],\n",
      "\n",
      "\n",
      "        [[[0.2098]]],\n",
      "\n",
      "\n",
      "        [[[0.2155]]],\n",
      "\n",
      "\n",
      "        [[[0.1896]]],\n",
      "\n",
      "\n",
      "        [[[0.2042]]],\n",
      "\n",
      "\n",
      "        [[[0.2151]]],\n",
      "\n",
      "\n",
      "        [[[0.1950]]],\n",
      "\n",
      "\n",
      "        [[[0.1807]]],\n",
      "\n",
      "\n",
      "        [[[0.1934]]],\n",
      "\n",
      "\n",
      "        [[[0.2104]]],\n",
      "\n",
      "\n",
      "        [[[0.1950]]],\n",
      "\n",
      "\n",
      "        [[[0.1794]]],\n",
      "\n",
      "\n",
      "        [[[0.2161]]],\n",
      "\n",
      "\n",
      "        [[[0.1787]]],\n",
      "\n",
      "\n",
      "        [[[0.2088]]],\n",
      "\n",
      "\n",
      "        [[[0.1900]]],\n",
      "\n",
      "\n",
      "        [[[0.1925]]],\n",
      "\n",
      "\n",
      "        [[[0.2055]]],\n",
      "\n",
      "\n",
      "        [[[0.2162]]],\n",
      "\n",
      "\n",
      "        [[[0.1994]]],\n",
      "\n",
      "\n",
      "        [[[0.1864]]],\n",
      "\n",
      "\n",
      "        [[[0.1882]]],\n",
      "\n",
      "\n",
      "        [[[0.1803]]],\n",
      "\n",
      "\n",
      "        [[[0.1983]]],\n",
      "\n",
      "\n",
      "        [[[0.1703]]],\n",
      "\n",
      "\n",
      "        [[[0.2192]]],\n",
      "\n",
      "\n",
      "        [[[0.1792]]],\n",
      "\n",
      "\n",
      "        [[[0.2276]]],\n",
      "\n",
      "\n",
      "        [[[0.2324]]],\n",
      "\n",
      "\n",
      "        [[[0.2096]]],\n",
      "\n",
      "\n",
      "        [[[0.2181]]],\n",
      "\n",
      "\n",
      "        [[[0.1932]]],\n",
      "\n",
      "\n",
      "        [[[0.2110]]],\n",
      "\n",
      "\n",
      "        [[[0.2004]]],\n",
      "\n",
      "\n",
      "        [[[0.2037]]],\n",
      "\n",
      "\n",
      "        [[[0.1966]]],\n",
      "\n",
      "\n",
      "        [[[0.2035]]],\n",
      "\n",
      "\n",
      "        [[[0.1903]]],\n",
      "\n",
      "\n",
      "        [[[0.2214]]],\n",
      "\n",
      "\n",
      "        [[[0.1635]]],\n",
      "\n",
      "\n",
      "        [[[0.1716]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "d_loss_real :  tensor(0.8025, grad_fn=<MeanBackward0>)\n",
      "d_out_real :  tensor([[[[0.2336]]],\n",
      "\n",
      "\n",
      "        [[[0.1982]]],\n",
      "\n",
      "\n",
      "        [[[0.2277]]],\n",
      "\n",
      "\n",
      "        [[[0.2194]]],\n",
      "\n",
      "\n",
      "        [[[0.2090]]],\n",
      "\n",
      "\n",
      "        [[[0.2371]]],\n",
      "\n",
      "\n",
      "        [[[0.2074]]],\n",
      "\n",
      "\n",
      "        [[[0.2239]]],\n",
      "\n",
      "\n",
      "        [[[0.2735]]],\n",
      "\n",
      "\n",
      "        [[[0.2366]]],\n",
      "\n",
      "\n",
      "        [[[0.2415]]],\n",
      "\n",
      "\n",
      "        [[[0.2273]]],\n",
      "\n",
      "\n",
      "        [[[0.2555]]],\n",
      "\n",
      "\n",
      "        [[[0.2300]]],\n",
      "\n",
      "\n",
      "        [[[0.2228]]],\n",
      "\n",
      "\n",
      "        [[[0.2142]]],\n",
      "\n",
      "\n",
      "        [[[0.2080]]],\n",
      "\n",
      "\n",
      "        [[[0.1931]]],\n",
      "\n",
      "\n",
      "        [[[0.2376]]],\n",
      "\n",
      "\n",
      "        [[[0.2126]]],\n",
      "\n",
      "\n",
      "        [[[0.1781]]],\n",
      "\n",
      "\n",
      "        [[[0.2213]]],\n",
      "\n",
      "\n",
      "        [[[0.2185]]],\n",
      "\n",
      "\n",
      "        [[[0.1699]]],\n",
      "\n",
      "\n",
      "        [[[0.2205]]],\n",
      "\n",
      "\n",
      "        [[[0.2498]]],\n",
      "\n",
      "\n",
      "        [[[0.1878]]],\n",
      "\n",
      "\n",
      "        [[[0.2342]]],\n",
      "\n",
      "\n",
      "        [[[0.2237]]],\n",
      "\n",
      "\n",
      "        [[[0.2457]]],\n",
      "\n",
      "\n",
      "        [[[0.2222]]],\n",
      "\n",
      "\n",
      "        [[[0.2376]]],\n",
      "\n",
      "\n",
      "        [[[0.2573]]],\n",
      "\n",
      "\n",
      "        [[[0.1670]]],\n",
      "\n",
      "\n",
      "        [[[0.2336]]],\n",
      "\n",
      "\n",
      "        [[[0.2234]]],\n",
      "\n",
      "\n",
      "        [[[0.2305]]],\n",
      "\n",
      "\n",
      "        [[[0.1929]]],\n",
      "\n",
      "\n",
      "        [[[0.2295]]],\n",
      "\n",
      "\n",
      "        [[[0.2255]]],\n",
      "\n",
      "\n",
      "        [[[0.2462]]],\n",
      "\n",
      "\n",
      "        [[[0.1908]]],\n",
      "\n",
      "\n",
      "        [[[0.2337]]],\n",
      "\n",
      "\n",
      "        [[[0.2221]]],\n",
      "\n",
      "\n",
      "        [[[0.2324]]],\n",
      "\n",
      "\n",
      "        [[[0.2425]]],\n",
      "\n",
      "\n",
      "        [[[0.2475]]],\n",
      "\n",
      "\n",
      "        [[[0.2033]]],\n",
      "\n",
      "\n",
      "        [[[0.2100]]],\n",
      "\n",
      "\n",
      "        [[[0.2029]]],\n",
      "\n",
      "\n",
      "        [[[0.2420]]],\n",
      "\n",
      "\n",
      "        [[[0.2429]]],\n",
      "\n",
      "\n",
      "        [[[0.2481]]],\n",
      "\n",
      "\n",
      "        [[[0.2423]]],\n",
      "\n",
      "\n",
      "        [[[0.1958]]],\n",
      "\n",
      "\n",
      "        [[[0.2206]]],\n",
      "\n",
      "\n",
      "        [[[0.2356]]],\n",
      "\n",
      "\n",
      "        [[[0.2393]]],\n",
      "\n",
      "\n",
      "        [[[0.2822]]],\n",
      "\n",
      "\n",
      "        [[[0.2248]]],\n",
      "\n",
      "\n",
      "        [[[0.2185]]],\n",
      "\n",
      "\n",
      "        [[[0.2283]]],\n",
      "\n",
      "\n",
      "        [[[0.2185]]],\n",
      "\n",
      "\n",
      "        [[[0.2207]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "d_loss_real :  tensor(0.7755, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_out_real :  tensor([[[[0.2456]]],\n",
      "\n",
      "\n",
      "        [[[0.2526]]],\n",
      "\n",
      "\n",
      "        [[[0.2587]]],\n",
      "\n",
      "\n",
      "        [[[0.2649]]],\n",
      "\n",
      "\n",
      "        [[[0.2527]]],\n",
      "\n",
      "\n",
      "        [[[0.2514]]],\n",
      "\n",
      "\n",
      "        [[[0.2438]]],\n",
      "\n",
      "\n",
      "        [[[0.2394]]],\n",
      "\n",
      "\n",
      "        [[[0.2192]]],\n",
      "\n",
      "\n",
      "        [[[0.2976]]],\n",
      "\n",
      "\n",
      "        [[[0.2774]]],\n",
      "\n",
      "\n",
      "        [[[0.2323]]],\n",
      "\n",
      "\n",
      "        [[[0.2690]]],\n",
      "\n",
      "\n",
      "        [[[0.2547]]],\n",
      "\n",
      "\n",
      "        [[[0.2505]]],\n",
      "\n",
      "\n",
      "        [[[0.1902]]],\n",
      "\n",
      "\n",
      "        [[[0.2706]]],\n",
      "\n",
      "\n",
      "        [[[0.2620]]],\n",
      "\n",
      "\n",
      "        [[[0.2602]]],\n",
      "\n",
      "\n",
      "        [[[0.2474]]],\n",
      "\n",
      "\n",
      "        [[[0.2026]]],\n",
      "\n",
      "\n",
      "        [[[0.2350]]],\n",
      "\n",
      "\n",
      "        [[[0.2570]]],\n",
      "\n",
      "\n",
      "        [[[0.2804]]],\n",
      "\n",
      "\n",
      "        [[[0.2474]]],\n",
      "\n",
      "\n",
      "        [[[0.2558]]],\n",
      "\n",
      "\n",
      "        [[[0.2493]]],\n",
      "\n",
      "\n",
      "        [[[0.2772]]],\n",
      "\n",
      "\n",
      "        [[[0.2613]]],\n",
      "\n",
      "\n",
      "        [[[0.2303]]],\n",
      "\n",
      "\n",
      "        [[[0.2681]]],\n",
      "\n",
      "\n",
      "        [[[0.2657]]],\n",
      "\n",
      "\n",
      "        [[[0.2729]]],\n",
      "\n",
      "\n",
      "        [[[0.2265]]],\n",
      "\n",
      "\n",
      "        [[[0.2316]]],\n",
      "\n",
      "\n",
      "        [[[0.2706]]],\n",
      "\n",
      "\n",
      "        [[[0.2521]]],\n",
      "\n",
      "\n",
      "        [[[0.2570]]],\n",
      "\n",
      "\n",
      "        [[[0.2770]]],\n",
      "\n",
      "\n",
      "        [[[0.2350]]],\n",
      "\n",
      "\n",
      "        [[[0.2604]]],\n",
      "\n",
      "\n",
      "        [[[0.2695]]],\n",
      "\n",
      "\n",
      "        [[[0.2296]]],\n",
      "\n",
      "\n",
      "        [[[0.2424]]],\n",
      "\n",
      "\n",
      "        [[[0.2499]]],\n",
      "\n",
      "\n",
      "        [[[0.2308]]],\n",
      "\n",
      "\n",
      "        [[[0.2940]]],\n",
      "\n",
      "\n",
      "        [[[0.2564]]],\n",
      "\n",
      "\n",
      "        [[[0.2258]]],\n",
      "\n",
      "\n",
      "        [[[0.2405]]],\n",
      "\n",
      "\n",
      "        [[[0.2559]]],\n",
      "\n",
      "\n",
      "        [[[0.2476]]],\n",
      "\n",
      "\n",
      "        [[[0.2607]]],\n",
      "\n",
      "\n",
      "        [[[0.2384]]],\n",
      "\n",
      "\n",
      "        [[[0.2483]]],\n",
      "\n",
      "\n",
      "        [[[0.3056]]],\n",
      "\n",
      "\n",
      "        [[[0.2437]]],\n",
      "\n",
      "\n",
      "        [[[0.2467]]],\n",
      "\n",
      "\n",
      "        [[[0.2394]]],\n",
      "\n",
      "\n",
      "        [[[0.2586]]],\n",
      "\n",
      "\n",
      "        [[[0.2603]]],\n",
      "\n",
      "\n",
      "        [[[0.2236]]],\n",
      "\n",
      "\n",
      "        [[[0.2534]]],\n",
      "\n",
      "\n",
      "        [[[0.2735]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "d_loss_real :  tensor(0.7477, grad_fn=<MeanBackward0>)\n",
      "d_out_real :  tensor([[[[0.2958]]],\n",
      "\n",
      "\n",
      "        [[[0.2719]]],\n",
      "\n",
      "\n",
      "        [[[0.2788]]],\n",
      "\n",
      "\n",
      "        [[[0.2611]]],\n",
      "\n",
      "\n",
      "        [[[0.3025]]],\n",
      "\n",
      "\n",
      "        [[[0.2902]]],\n",
      "\n",
      "\n",
      "        [[[0.2727]]],\n",
      "\n",
      "\n",
      "        [[[0.3200]]],\n",
      "\n",
      "\n",
      "        [[[0.2618]]],\n",
      "\n",
      "\n",
      "        [[[0.3051]]],\n",
      "\n",
      "\n",
      "        [[[0.2725]]],\n",
      "\n",
      "\n",
      "        [[[0.2322]]],\n",
      "\n",
      "\n",
      "        [[[0.2742]]],\n",
      "\n",
      "\n",
      "        [[[0.2166]]],\n",
      "\n",
      "\n",
      "        [[[0.2832]]],\n",
      "\n",
      "\n",
      "        [[[0.2445]]],\n",
      "\n",
      "\n",
      "        [[[0.2591]]],\n",
      "\n",
      "\n",
      "        [[[0.2498]]],\n",
      "\n",
      "\n",
      "        [[[0.3019]]],\n",
      "\n",
      "\n",
      "        [[[0.2535]]],\n",
      "\n",
      "\n",
      "        [[[0.2390]]],\n",
      "\n",
      "\n",
      "        [[[0.2741]]],\n",
      "\n",
      "\n",
      "        [[[0.2575]]],\n",
      "\n",
      "\n",
      "        [[[0.2373]]],\n",
      "\n",
      "\n",
      "        [[[0.2653]]],\n",
      "\n",
      "\n",
      "        [[[0.3013]]],\n",
      "\n",
      "\n",
      "        [[[0.2798]]],\n",
      "\n",
      "\n",
      "        [[[0.2432]]],\n",
      "\n",
      "\n",
      "        [[[0.2591]]],\n",
      "\n",
      "\n",
      "        [[[0.1932]]],\n",
      "\n",
      "\n",
      "        [[[0.2811]]],\n",
      "\n",
      "\n",
      "        [[[0.2705]]],\n",
      "\n",
      "\n",
      "        [[[0.2674]]],\n",
      "\n",
      "\n",
      "        [[[0.2339]]],\n",
      "\n",
      "\n",
      "        [[[0.3170]]],\n",
      "\n",
      "\n",
      "        [[[0.2477]]],\n",
      "\n",
      "\n",
      "        [[[0.2709]]],\n",
      "\n",
      "\n",
      "        [[[0.2615]]],\n",
      "\n",
      "\n",
      "        [[[0.2638]]],\n",
      "\n",
      "\n",
      "        [[[0.2453]]],\n",
      "\n",
      "\n",
      "        [[[0.2839]]],\n",
      "\n",
      "\n",
      "        [[[0.2639]]],\n",
      "\n",
      "\n",
      "        [[[0.2622]]],\n",
      "\n",
      "\n",
      "        [[[0.2632]]],\n",
      "\n",
      "\n",
      "        [[[0.2555]]],\n",
      "\n",
      "\n",
      "        [[[0.2776]]],\n",
      "\n",
      "\n",
      "        [[[0.2628]]],\n",
      "\n",
      "\n",
      "        [[[0.2642]]],\n",
      "\n",
      "\n",
      "        [[[0.2693]]],\n",
      "\n",
      "\n",
      "        [[[0.2822]]],\n",
      "\n",
      "\n",
      "        [[[0.2672]]],\n",
      "\n",
      "\n",
      "        [[[0.2603]]],\n",
      "\n",
      "\n",
      "        [[[0.3055]]],\n",
      "\n",
      "\n",
      "        [[[0.3059]]],\n",
      "\n",
      "\n",
      "        [[[0.2535]]],\n",
      "\n",
      "\n",
      "        [[[0.2549]]],\n",
      "\n",
      "\n",
      "        [[[0.2914]]],\n",
      "\n",
      "\n",
      "        [[[0.3143]]],\n",
      "\n",
      "\n",
      "        [[[0.2040]]],\n",
      "\n",
      "\n",
      "        [[[0.2699]]],\n",
      "\n",
      "\n",
      "        [[[0.3044]]],\n",
      "\n",
      "\n",
      "        [[[0.3136]]],\n",
      "\n",
      "\n",
      "        [[[0.2741]]],\n",
      "\n",
      "\n",
      "        [[[0.3243]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "d_loss_real :  tensor(0.7299, grad_fn=<MeanBackward0>)\n",
      "d_out_real :  tensor([[[[0.3327]]],\n",
      "\n",
      "\n",
      "        [[[0.3073]]],\n",
      "\n",
      "\n",
      "        [[[0.3206]]],\n",
      "\n",
      "\n",
      "        [[[0.2998]]],\n",
      "\n",
      "\n",
      "        [[[0.2824]]],\n",
      "\n",
      "\n",
      "        [[[0.3240]]],\n",
      "\n",
      "\n",
      "        [[[0.3199]]],\n",
      "\n",
      "\n",
      "        [[[0.2330]]],\n",
      "\n",
      "\n",
      "        [[[0.2929]]],\n",
      "\n",
      "\n",
      "        [[[0.3332]]],\n",
      "\n",
      "\n",
      "        [[[0.2986]]],\n",
      "\n",
      "\n",
      "        [[[0.2875]]],\n",
      "\n",
      "\n",
      "        [[[0.3238]]],\n",
      "\n",
      "\n",
      "        [[[0.2757]]],\n",
      "\n",
      "\n",
      "        [[[0.2761]]],\n",
      "\n",
      "\n",
      "        [[[0.3346]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "d_loss_real :  tensor(0.6974, grad_fn=<MeanBackward0>)\n",
      "----------------------------\n",
      "epoch 1 || Epoch_D_Loss:0.1981 ||Epoch_G_Loss:-0.0038\n",
      "timer:  164.9897 sec.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "G_update, D_update = train_model(\n",
    "    G,D,dataloader=train_dataloader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
