{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANによる画像生成\n",
    "\n",
    "commit用\n",
    "\n",
    "```mycode/5_gan_generation/```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: gpustat: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-3 Self-Attention GANの概要\n",
    "\n",
    "2018年時点で最高峰のGANの１つである**BigGAN**もSAGANをベースとしているGANである．本節では\n",
    "\n",
    "- Self-Attention\n",
    "- pointwise convolution\n",
    "- Spectral Normalization\n",
    "\n",
    "の３つのテーマについての理解を目標とする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 従来のGANの問題点\n",
    "従来のGANの転置畳み込みを繰り返す操作は特徴量マップが大きくなりますが，転置畳み込みの繰り返しでは**局所的な情報の拡大にしかならない**問題がある．\n",
    "\n",
    "よってより良い画像生成を実現するためには可能であれば拡大する際に画像全体の大域的な情報を考慮する仕組みが必要になる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attentionの導入\n",
    "従来のニューラルネットワークでは前の層の出力がそのまま次の層に入力されるので式としては\n",
    "$$\n",
    "y = x\n",
    "$$\n",
    "で表される．しかしこれでは局所的な性質が常に伝わっていくので，さらに大域的な情報を用いて表される$o$を導入する．この$o$に係数$\\gamma$をかけた値を足して次の層に伝達することを考える．\n",
    "$$\n",
    "y = x +\\gamma o\n",
    "$$\n",
    "この$o$をどのように求めるかについては以下の手順で求める．\n",
    "1. $x$を[C,W,H]から[C,N]に変形する. $N=W\\times H$\n",
    "2. $S=x^Tx$ という行列を用意する．これは画像位置$i$と画像位置$j$の関係性を表す行列である．\n",
    "3. $S$を行方向にソフトマックス関数にかける．これで画像位置$i$と画像位置$j$の関係性の総和が1になり扱いやすくなる．これを$\\beta$とし，これを**Attention Map**という．\n",
    "4. 上で作った$\\beta$を$x$にかけることで$o$を得る．つまり$o = x\\beta^T$である．$o=[C,N]$\n",
    "\n",
    "この$o = x\\beta^T$について\n",
    "$$\n",
    "o_{c=k,n=j} = \\sum_{i=1}^Nx_{ki}\\beta_{ji}\n",
    "$$\n",
    "この式はチャネル$k$において位置$j$について位置$i=1,2,\\cdots N$の全ての影響を総和することを意味している．\n",
    "$x_{ki}$ はチャネル$k$の位置$i$の特徴量を表していて，$\\beta_{ji}$は位置$j$と位置$i$の関係性を表して，この$\\sum$はそれらの積の総和を求めている．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　最初のサイズ変形 (B,C,W,H)  to (B,C,N)\n",
    "X = torch.randn(3,32,64,64)\n",
    "X = X.view(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "# 掛け算\n",
    "X_T = X.permute(0,2,1)\n",
    "S = torch.bmm(X_T, X)\n",
    "\n",
    "# 規格化\n",
    "m = nn.Softmax(dim=-2)\n",
    "attention_map_T = m(S)\n",
    "attention_map = attention_map_T.permute(0,2,1)\n",
    "\n",
    "# Self-Attention Map\n",
    "o = torch.bmm(X, attention_map.permute(0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1×1 Convolutions (pointwise convolution)\n",
    "Self-Attention の制限は非常に強いのでこのままでは学習はうまくいかないことが多い．\n",
    "そこでpointwise convolutionと呼ばれる前処理をした特徴量マップをSelf-Attentionに渡してあげることによって学習がうまくいくようにする．\n",
    "またこれをすることによって特徴量マップのチャネル数を変えることができる(一般にはチャネル数を減らす)ので計算量を調整することができる．\n",
    "\n",
    "またpointwise convolutionでは次の言葉が使われる\n",
    "- query：元の入力$x$の転置に対応するもの\n",
    "- key：元の入力$x$に対応するもの\n",
    "- value：Attention Mapと掛け算する対象\n",
    "\n",
    "出力のチャネルはカーネルの数となる．カーネルのチャネルは入力のチャネルに合わせる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(3,32,64,64)\n",
    "\n",
    "# 1×1の畳み込み層によるpointwise convolutionを用意　\n",
    "query_conv = nn.Conv2d(\n",
    "    in_channels=X.shape[1], out_channels=X.shape[1]//8, kernel_size=1)\n",
    "\n",
    "key_conv = nn.Conv2d(\n",
    "    in_channels=X.shape[1], out_channels=X.shape[1]//8, kernel_size=1)\n",
    "\n",
    "value_conv = nn.Conv2d(\n",
    "    in_channels=X.shape[1], out_channels=X.shape[1], kernel_size=1)\n",
    "\n",
    "# 畳み込みをしてからサイズを変更する\n",
    "proj_query = query_conv(X).view(\n",
    "    X.shape[0], -1, X.shape[2]*X.shape[3])\n",
    "proj_query = proj_query.permute(0,2,1)\n",
    "proj_key = key_conv(X).view(\n",
    "    X.shape[0], -1, X.shape[2]*X.shape[3])\n",
    "\n",
    "# 掛け算\n",
    "S = torch.bmm(proj_query, proj_key) # bmmはバッチごとの掛け算を行う\n",
    "\n",
    "# 規格化\n",
    "m = nn.Softmax(dim=-2)\n",
    "attention_map_T = m(S)\n",
    "attention_map = attention_map_T.permute(0,2,1)\n",
    "\n",
    "# Self-Attention Map\n",
    "proj_value = value_conv(X).view(\n",
    "    X.shape[0], -1, X.shape[2]*X.shape[3])\n",
    "o = torch.bmm(proj_value, attention_map.permute(0,2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Normalization\n",
    "- Spectral Normalization：畳み込みの重みに対する規格化\n",
    "- Batch Normalization：ディープラーニングモデルを流れるデータに対する規格化\n",
    "\n",
    "***リプシッツ連続性(Lipschitz continuity)：***判別機Dの出力が入力データに対して頑健性を持つこと\n",
    "具体的には層を表す行列の固有値のうち最大の固有値で全ての重みを割ることで規格化している．固有値は固有ベクトルの拡大率なので，一番拡大される固有値で全てを規格化することによって流れるデータが想定外に大きくなることを防ぐ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvTranspose2d(20, 512, kernel_size=(4, 4), stride=(1, 1))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_dim = 20\n",
    "image_size = 64\n",
    "nn.utils.spectral_norm(nn.ConvTranspose2d(\n",
    "    z_dim, image_size*8, kernel_size=4,stride=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
