{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# セマンティックセグメンテーション(PSPNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: gpustat: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# 実装したネットワークで出力\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup seeds\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1 セマンティックセグメンテーションとは\n",
    "本章では画像処理タスクの１つである，セマンティックセグメンテーションに取り組みながら**PSPNet(Pyramid Scene Parsing Network)**と呼ばれるモデルについて理解する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### セマンティックセグメンテーションの入出力\n",
    "セマンティックでの入出力の関係は\n",
    "- input : 画像\n",
    "- output : 各ピクセルが所属するクラスのラベル情報\n",
    "\n",
    "出力については**カラーパレット形式**よって表現し，それぞれのラベルに対して予め固有のRGBの値を付与しておきそのRGBの値を元に図示する."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOCデータセット\n",
    "ここではVOCデータセットを用いるが，その中でもセマンティックセグメンテーション用にアノテーションが用意されている画像データのみを使用する．訓練データ1,464枚と検証データ1,449枚を用いる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSPNetによる物体検出の流れ\n",
    "**PSPNet(Pyramid Scene Parsing Network)** とはセマンティックセグメンテーションを行うディープラーニングアルゴリズムの一種である．\n",
    "\n",
    "PSPNetは大きく４つのステップによって構成される．\n",
    "1. 前処理を行う．具体的には画像サイズのリサイズと色情報の標準化を行う.\n",
    "2. PSPNetに対して前処理した画像を入力する(H,W)．すると出力として(C+1, H, W)のサイズを持つ出力を得る．C+1は背景を加えたクラスの数を表している．この出力は各ピクセルに対するそれぞれのクラスの確信度を表す．\n",
    "3. PSPNetの出力に対して，各ピクセルのクラス予測確信度が最大のものをそのピクセルのクラスと予測する．\n",
    "4. 3の処理を終えた(H, W)のサイズの画像を元の画像サイズにリサイズし直して出力する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2 DatasetとDataLoaderの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像データ，アノテーションデータへのファイルパスのリストを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datapath_list(rootpath):\n",
    "    \n",
    "    \n",
    "    # テンプレート この%sにあとでいろいろ入れる\n",
    "    imgpath_template = osp.join(rootpath, \"JPEGImages\", \"%s.jpg\")\n",
    "    annopath_template = osp.join(rootpath, \"SegmentationClass\", \"%s.png\")\n",
    "    \n",
    "    # 訓練と検証，　それぞれのファイルIDを取得\n",
    "    train_id_names = osp.join(rootpath + \"ImageSets/Segmentation/train.txt\")\n",
    "    val_id_names = osp.join(rootpath + \"ImageSets/Segmentation/val.txt\")\n",
    "    \n",
    "    # 訓練データの画像ファイルとアノテーションファイルへのパスリスト\n",
    "    train_img_list = list()\n",
    "    train_anno_list = list()\n",
    "    \n",
    "    for line in open(train_id_names):\n",
    "        file_id = line.strip()\n",
    "        img_path = (imgpath_template % file_id)\n",
    "        anno_path = (annopath_template % file_id)\n",
    "        train_img_list.append(img_path)\n",
    "        train_anno_list.append(anno_path)\n",
    "        \n",
    "    # 検証データの画像ファイルとアノテーションファイルへのパスリスト\n",
    "    val_img_list = list()\n",
    "    val_anno_list = list()\n",
    "    \n",
    "    for line in open(val_id_names):\n",
    "        file_id = line.strip()\n",
    "        img_path = (imgpath_template % file_id)\n",
    "        anno_path = (annopath_template % file_id)\n",
    "        val_img_list.append(img_path)\n",
    "        val_anno_list.append(anno_path)\n",
    "        \n",
    "    return train_img_list, train_anno_list, val_img_list, val_anno_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 動作確認\n",
    "# roopath = \".data/VOCdevkit/VOC2012/\"\n",
    "\n",
    "# train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(roopath)\n",
    "# print(train_img_list[0])\n",
    "# print(train_anno_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasetの作成\n",
    "まずDataTransformを実装する．このDataTransformの詳しい内容については省略し，前処理クラスのインポートを行うことにとどめる．\n",
    "\n",
    "1. まず対象画像とアノテーションデータをセットに対してセットで前処理を行う必要があるのでこれらをセットで変換することを可能にする```Compose```を用意する．\n",
    "2. 次にこの```Compose```クラス内で種々の前処理を行うことによってモデルの汎化性能を高める．ここで行う前処理はリサイズ・切り取り・回転・色情報の標準化などがある．\n",
    "\n",
    "セグメンテーション用のアノテーションデータには，物体の境界部分にはラベル255が特別についているのでこれは背景を表すラベル０に吸収させる．物体検出では背景に元々ラベルが与えられたなかったので全体のインデックスをインクリメントする必要があったがセグメンテーション用のアノテーションではそのようなことをする必要はない．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
