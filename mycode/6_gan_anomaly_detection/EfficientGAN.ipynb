{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANによる異常検知"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: gpustat: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッケージのimport\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup seeds\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-3 Efficient GANの概要\n",
    "AnoGANではテスト画像に対して異常検知をするのにまずテスト画像と最も近い画像を作ることのできる生成ノイズを学習する必要があり，異常検知に時間がかかってしまう問題があった．本節ではその問題を解決するためにEfficient GANを理解する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient GAN\n",
    "Efficient GANではテスト画像から生成ノイズzを生成するエンコーダEを新たに作る．つまりこれは生成器Gとは逆の作用をしていて${\\rm G}^{-1}$と表すことができる．このEについては通常通りGANを生成した後に独立に作ろうとすると上手くいかないらしい．なのでここではG,D,Eを同時に作る手順を理解する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoderを作る方法，後から作る作戦が上手くいかない理由\n",
    "- [Adversarially learned inference](https://arxiv.org/pdf/1606.00704.pdf)\n",
    "- [Adversarially learned inference GitHub page](https://ishmaelbelghazi.github.io/ALI/)\n",
    "\n",
    "\n",
    "詳しくは上のサイトの図を参照するのがわかりやすい．それぞれの方法で作成したEncoderについて二次元データ(本来は画像を用いるところ)を入力して，生成ノイズzの空間に写像させた時に本来GANの入力ノイズは平均0,　分散1の均一な空間になると基づいて設計されているが適切な学習をさせていないEncoderの写像はそのような空間になっていない．つまりこれは平均0, 分散1の生成ノイズに対して上手く画像を生成できないことを意味している．\n",
    "\n",
    "この直感的な理由の理解としては，Generatorが入力データの平均や分散を完全に理解していれば問題ないがそれは現実的には困難であり，不完全に学習したGeneratorを基にして学習するよりは本来学習の基にすべきである教師データを基にEncoderも学習するべきであるというように考えることができる．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EncoderをGANと同時に作る方法\n",
    "今回はEncoderを教師データの画像xと関与させるために**BiGAN(Bidiretional GAN)** と呼ばれる仕組みを利用する．\n",
    "BiGANではDiscriminatorに対して$(x, E(x))$と$(G(z), z)$を入力する．\n",
    "\n",
    "つまり元々のDCGANの損失関数とBiGANの損失関数を比較すると\n",
    "- Discriminator\n",
    "$$\n",
    "-\\sum_{i=1}^M\\log D(x) -\\sum_{j=1}^M \\log(1-D(G(z))\n",
    "$$\n",
    "$$\n",
    "-\\sum_{i=1}^M[\\log  D(x_i, E(x_i)-\\sum_{j=1}^M  \\log(1-D(G(z_j), z_j))]\n",
    "$$\n",
    "\n",
    "- Generator\n",
    "$$\n",
    "-\\sum_{j=1}^M\\log D(G(z_j))\n",
    "$$\n",
    "$$\n",
    "-\\sum_{j=1}^M\\log D(G(z_j), z_j)\n",
    "$$\n",
    "\n",
    "ここでEncoderの損失関数を考えると，EncoderはDiscriminatorを騙せると嬉しいので，Discriminatorの損失関数にマイナスをかけてEncoderに関与するところだけを取り出すと\n",
    "- Encoder\n",
    "\n",
    "$\\sum_{i=1}^M\\log D(x_i, E(x_i))$\n",
    "となる．しかしGeneratorと同様にこのままでは学習が進まないので以下のようにする．\n",
    "$$\n",
    "- \\sum_{i=1}^M\\log (1- D(x_i, E(x_i)))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 誤差関数の実装のイメージ\n",
    "# BCEwithLogitsLossは入力にシグモイドを自動でかけてくれる\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "# データローダーからmini-batchごとに取り出す\n",
    "for imges in dataloader:\n",
    "    label_real = torch.full((mini_batch_size,),1)\n",
    "    label_fake = torch.full((mini_batch_size,),0)\n",
    "    \n",
    "    \"\"\"D\"\"\"\n",
    "    # 前半部分\n",
    "    z_out_real = E(imges)\n",
    "    d_out_real, _ = D(imges, z_out_real)\n",
    "    \n",
    "    #　後半部分\n",
    "    input_z = torch.randn(mini_batch_size, z_dim).to(device)\n",
    "    fake_images = G(input_z)\n",
    "    d_out_fake, _ = D(fake_images, input_z)\n",
    "    \n",
    "    # 誤差の計算\n",
    "    d_loss_real = criterion(d_out_real.view(-1), label_real)\n",
    "    d_loss_fake = criterion(d_out_fake.view(-1), label_fake)\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    \n",
    "    #  backward\n",
    "    d_optimizer.zero_grad()\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    \n",
    "    \"\"\"G\"\"\"\n",
    "    # 偽画像を生成して判定\n",
    "    input_z = torch.randn(mini_batch_size, z_dim).to(device)\n",
    "    fake_images = G(input_z)\n",
    "    d_out_fake, _ = D(fake_images, input_z)\n",
    "    \n",
    "    # 誤差の計算\n",
    "    g_loss = criterion(d_out_fake.view(-1), label_real)\n",
    "    \n",
    "    # backward\n",
    "    g_optimizer.zero_grad()\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    \n",
    "    \n",
    "    \"\"\"E\"\"\"\n",
    "    z_out_real = E(imges)\n",
    "    d_out_real, _ = D(imges, z_out_real)\n",
    "    \n",
    "    # 誤差の計算\n",
    "    e_loss = criterion(d_out_real.view(-1), label_fake)\n",
    "    \n",
    "    # backward\n",
    "    e_optimizer.zero_grad()\n",
    "    e_loss.backward()\n",
    "    e_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-4 Efficient GANの実装と異常検知の実施\n",
    "mnistの7と8の画像を正常画像としてGANを学習する．この時画像サイズが64ピクセル平方ではなく28ピクセル平方であることに注意が必要．Encoderも同時に学習させることも含めて前に作ったGANの学習ずみモデルをここに適用することができないのでサイド学習させる必要がある．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeneratorとDiscriminatorの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_dim=20):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(z_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(1024, 7*7*128),\n",
    "            nn.BatchNorm1d(7*7*128),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.last = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh())\n",
    "        \n",
    "        \n",
    "    def forward(self, z):\n",
    "        out = self.layer1(z)\n",
    "        out = self.layer2(out)\n",
    "        \n",
    "        # 転置畳み込み層にいれるテンソルに形を変形する\n",
    "        out = out.view(z.shape[0], 128, 7, 7)\n",
    "        out = self.layer3(out)\n",
    "        out = self.last(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 動作確認\n",
    "G = Generator(z_dim=20)\n",
    "G.train()\n",
    "\n",
    "# Batch_Normがあるのでミニバッチは２以上の数\n",
    "input_z = torch.randn(2,20)\n",
    "\n",
    "fake_images = G(input_z)\n",
    "img_transformed = fake_images[0][0].detach().numpy()\n",
    "plt.imshow(img_transformed,\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiGANのモデルでは識別器Dに対して画像データxだけではなく入力ノイズzも入力します．これら２つは畳み込み層と全結合層で別々に処理された後に結合される．結合には```torch.cat()```を利用し，その後再度全結合層にて処理をする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_dim=20):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # 入力画像の処理\n",
    "        self.x_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1), # 白黒なのでinput_channel = 1\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "        \n",
    "        self.x_layer2 =  nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1), # 白黒なのでinput_channel = 1\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "        \n",
    "        # 乱数の入力処理\n",
    "        self.z_layer1 = nn.Linear(z_dim, 512)\n",
    "        \n",
    "        # 最後の判定\n",
    "        self.last1 = nn.Sequential(\n",
    "            nn.Linear(3648, 1024),  # (3648 - 512)**0.5 = 56\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "\n",
    "        self.last2 = nn.Linear(1024, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, z):\n",
    "        \n",
    "        # 画像\n",
    "        x_out = self.x_layer1(x)\n",
    "        x_out = self.x_layer2(x_out)\n",
    "        \n",
    "        # 乱数\n",
    "        z = z.view(z.shape[0],-1)\n",
    "        z_out = self.z_layer1(z)\n",
    "        \n",
    "        # x_out と z_out の結合 \n",
    "        # その後全結合層\n",
    "        x_out = x_out.view(-1, 64*7*7)\n",
    "        out = torch.cat([x_out, z_out], dim=1)\n",
    "        out = self.last1(out)\n",
    "        \n",
    "        feature = out\n",
    "        feature = feature.view(feature.size()[0], -1)\n",
    "        \n",
    "        out = self.last2(out)\n",
    "        \n",
    "        return out , feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4644],\n",
      "        [0.5130]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 動作確認\n",
    "D = Discriminator(z_dim=20)\n",
    "\n",
    "input_z = torch.randn(2,20)\n",
    "fake_images = G(input_z)\n",
    "\n",
    "d_out, _ = D(fake_images, input_z)\n",
    "\n",
    "print(nn.Sigmoid()(d_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoderの実装\n",
    "出力サイズが一次元ではなく```z_dim```になることに注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_dim=20):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1), # 白黒なので　input_channel = 1\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "        \n",
    "        # ここまでで画像サイズが7×7\n",
    "        self.last = nn.Linear(128*7*7, z_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        \n",
    "        # LInearに入力するために整形する\n",
    "        out = out.view(-1, 128*7*7)\n",
    "        out = self.last(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20])\n",
      "tensor([[ 0.1963, -0.3740, -0.5804,  0.6870, -0.0237, -0.9918, -0.3687, -0.4212,\n",
      "         -0.2987, -0.2438, -0.0394, -0.1112, -0.1076,  0.5265, -0.0333, -0.2125,\n",
      "          0.4872, -0.4294, -0.5205,  0.0964],\n",
      "        [-0.1606,  0.2021, -0.1141,  0.1908,  0.4037,  0.2272, -0.4661,  0.0418,\n",
      "          0.1836, -0.5176, -0.5233, -0.1162,  0.5253,  0.2757, -0.3043, -0.7173,\n",
      "          0.5581,  0.3669, -0.4870,  0.2635]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 動作確認\n",
    "E = Encoder(z_dim=20)\n",
    "\n",
    "x = fake_images # とりあえず代用\n",
    "\n",
    "z = E(x)\n",
    "\n",
    "print(z.shape)\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaderの実装\n",
    "DCGANのDataLoaderと同じだが画像サイズが異なるのでパスだけ変える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datapath_list():\n",
    "    \n",
    "    train_img_list = list()\n",
    "    \n",
    "    for img_idx in range(200):\n",
    "        img_path = \"./data/img_78_28size/img_7_\" + str(img_idx) + \".jpg\"\n",
    "        train_img_list.append(img_path)\n",
    "        \n",
    "        img_path = \"./data/img_78_28size/img_8_\" + str(img_idx) + \".jpg\"\n",
    "        train_img_list.append(img_path)\n",
    "        \n",
    "    return train_img_list\n",
    "\n",
    "\n",
    "class ImageTransform():\n",
    "    \"\"\"\n",
    "    画像クラスの前処理\n",
    "    \"\"\"\n",
    "    def __init__(self, mean, std):\n",
    "        self.data_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        return self.data_transform(img)\n",
    "    \n",
    "\n",
    "class GAN_Img_Dataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    画像のDataset\n",
    "    \"\"\"\n",
    "    def __init__(self, file_list, transform):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\"画像の枚数を返す\"\"\"\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \"\"\"前処理をした画像のTensor形式のデータを取得\"\"\"\n",
    "        \n",
    "        img_path = self.file_list[index]\n",
    "        img = Image.open(img_path) #H,W,C\n",
    "        \n",
    "        # 前処理\n",
    "        img_transformed = self.transform(img)\n",
    "        \n",
    "        return img_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderの作成\n",
    "train_img_list = make_datapath_list()\n",
    "# print(train_img_list)\n",
    "\n",
    "# Datasetの作成\n",
    "mean = (0.5,)\n",
    "std = (0.5,)\n",
    "train_dataset = GAN_Img_Dataset(train_img_list, ImageTransform(mean, std))\n",
    "\n",
    "# DataLoaderの作成\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                              batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#動作確認\n",
    "batch_iterator = iter(train_dataloader)\n",
    "imges = next(batch_iterator)\n",
    "print(imges.size())\n",
    "img_transformed = imges[0][0].detach().numpy()\n",
    "plt.imshow(img_transformed,'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient GANの学習\n",
    "Encoderを追加する以外はDCGANと同じであるが，Discriminatorの学習率を低めに設定してある．これはDiscriminatorはGeneratorとEncoderの２つに対して学習をするので学習する機会が多いので学習率を下げることによって１つだけが学習が過度に進んで全体として学習が進まなくなるのを防ぐ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(G, D, E, dataloader, num_epochs):\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス : \", device)\n",
    "    \n",
    "    # 最適化手法の適用\n",
    "    lr_ge = 0.0001\n",
    "    lr_d = lr_ge/4.0\n",
    "    beta1, beta2 = 0.5, 0.999\n",
    "    g_optimizer = torch.optim.Adam(G.parameters(), lr_ge, [beta1, beta2])\n",
    "    d_optimizer = torch.optim.Adam(D.parameters(), lr_d, [beta1, beta2])\n",
    "    e_optimizer = torch.optim.Adam(E.parameters(), lr_ge, [beta1, beta2])\n",
    "    \n",
    "    # 損失関数\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "    \n",
    "    z_dim = 20\n",
    "    mini_batch_size = 64\n",
    "    \n",
    "    G.to(device)\n",
    "    D.to(device)\n",
    "    E.to(device)\n",
    "    \n",
    "    G.train()\n",
    "    D.train()\n",
    "    E.train()\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    num_train_imgs = len(dataloader.dataset)\n",
    "    batch_size = dataloader.batch_size\n",
    "    \n",
    "    # イテレーションのカウンタをセット\n",
    "    iteration = 1\n",
    "    logs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        t_epoch_start = time.time()\n",
    "        epoch_g_loss = 0.0\n",
    "        epoch_d_loss = 0.0\n",
    "        epoch_e_loss = 0.0\n",
    "        if ((epoch+1)%100==0):\n",
    "            print(\"-----------------------------------\")\n",
    "            print(\"Epoch {}/{} \".format(epoch, num_epochs))\n",
    "            print(\"-----------------------------------\")\n",
    "            print(\"---  (train)  ---\")\n",
    "\n",
    "        # dataloader からmini_batchずつデータを取り出す\n",
    "        for imges in dataloader:\n",
    "            \n",
    "            # batch_norm の　エラーを回避\n",
    "            if imges.size()[0] == 1:\n",
    "                continue\n",
    "                \n",
    "            \n",
    "            mini_batch_size = imges.size()[0]\n",
    "            label_real = torch.full((mini_batch_size,),1).to(device)\n",
    "            label_fake = torch.full((mini_batch_size,),0).to(device)\n",
    "            \n",
    "            imges = imges.to(device)\n",
    "            \n",
    "            \n",
    "            \"\"\"D\"\"\"\n",
    "            # 前半部分\n",
    "            z_out_real = E(imges)\n",
    "            d_out_real, _ = D(imges, z_out_real)\n",
    "\n",
    "            #　後半部分\n",
    "            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n",
    "            fake_images = G(input_z)\n",
    "            d_out_fake, _ = D(fake_images, input_z)\n",
    "\n",
    "            # 誤差の計算\n",
    "            d_loss_real = criterion(d_out_real.view(-1), label_real)\n",
    "            d_loss_fake = criterion(d_out_fake.view(-1), label_fake)\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "        \n",
    "            #  backward\n",
    "            d_optimizer.zero_grad()\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "\n",
    "            \"\"\"G\"\"\"\n",
    "            # 偽画像を生成して判定\n",
    "            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n",
    "            fake_images = G(input_z)\n",
    "            d_out_fake, _ = D(fake_images, input_z)\n",
    "\n",
    "            # 誤差の計算\n",
    "            g_loss = criterion(d_out_fake.view(-1), label_real)\n",
    "\n",
    "            # backward\n",
    "            g_optimizer.zero_grad()\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "             \n",
    "            \n",
    "            \"\"\"E\"\"\"\n",
    "            z_out_real = E(imges)\n",
    "            d_out_real, _ = D(imges, z_out_real)\n",
    "\n",
    "            # 誤差の計算\n",
    "            e_loss = criterion(d_out_real.view(-1), label_fake)\n",
    "\n",
    "            # backward\n",
    "            e_optimizer.zero_grad()\n",
    "            e_loss.backward()\n",
    "            e_optimizer.step()\n",
    "            \n",
    "            \n",
    "            \"\"\"記録\"\"\"\n",
    "            epoch_d_loss += d_loss.item()\n",
    "            epoch_e_loss += e_loss.item()\n",
    "            epoch_g_loss += g_loss.item()\n",
    "        \n",
    "        if ((epoch+1)%100==0):\n",
    "            t_epoch_finish = time.time()\n",
    "            print('-------------')\n",
    "            print('epoch {} || Epoch_D_Loss:{:.4f} ||Epoch_G_Loss:{:.4f} ||Epoch_E_Loss:{:.4f}'.format(\n",
    "                epoch, epoch_d_loss/batch_size, epoch_g_loss/batch_size, epoch_e_loss/batch_size))\n",
    "            print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "            t_epoch_start = time.time()\n",
    "            \n",
    "            if ((epoch)%num_epochs==0):\n",
    "                torch.save(G.state_dict(), \"weights/bigan_g_\"+str(epoch+1) + \".pth\")\n",
    "                torch.save(D.state_dict(), \"weights/bigan_d_\"+str(epoch+1) + \".pth\")\n",
    "                torch.save(E.state_dict(), \"weights/bigan_e_\"+str(epoch+1) + \".pth\")\n",
    "            \n",
    "            \n",
    "    print(\"総イテレーション回数:\", iteration)\n",
    "    \n",
    "    return G, D, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワークの初期化\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "        \n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ネットワークの初期化完了\n"
     ]
    }
   ],
   "source": [
    "init_flag = 0\n",
    "model_epoch_num = 300\n",
    "\n",
    "G = Generator(z_dim = 20)\n",
    "D = Discriminator(z_dim = 20)\n",
    "E = Encoder(z_dim = 20)\n",
    "\n",
    "if init_flag == 0:\n",
    "\n",
    "    # 初期化の実施\n",
    "    G.apply(weights_init)\n",
    "    E.apply(weights_init)\n",
    "    D.apply(weights_init)\n",
    "\n",
    "    print(\"ネットワークの初期化完了\")\n",
    "    \n",
    "else:\n",
    "\n",
    "#     学習済みモデルの適用\n",
    "#     G.to(device)\n",
    "#     D.to(device)\n",
    "#     E.to(device)\n",
    "\n",
    "    g_weights = torch.load(\"weights/bigan_g_\"+ str(model_epoch_num)+ \".pth\",\n",
    "                            map_location={'cuda:0': 'cpu'})\n",
    "    d_weights = torch.load(\"weights/bigan_d_\"+ str(model_epoch_num)+ \".pth\",\n",
    "                            map_location={'cuda:0': 'cpu'})\n",
    "    e_weights = torch.load(\"weights/bigan_d_\"+ str(model_epoch_num)+ \".pth\",\n",
    "                            map_location={'cuda:0': 'cpu'})\n",
    "\n",
    "\n",
    "    G.load_state_dict(g_weights)\n",
    "    D.load_state_dict(d_weights)\n",
    "    E.load_state_dict(e_weights)\n",
    "\n",
    "    print(\"ネットワークのモデル適用完了\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス :  cpu\n",
      "-----------------------------------\n",
      "Epoch 0/50 \n",
      "-----------------------------------\n",
      "---  (train)  ---\n",
      "-----------------------------------\n",
      "Epoch 1/50 \n",
      "-----------------------------------\n",
      "---  (train)  ---\n",
      "-----------------------------------\n",
      "Epoch 2/50 \n",
      "-----------------------------------\n",
      "---  (train)  ---\n",
      "-----------------------------------\n",
      "Epoch 3/50 \n",
      "-----------------------------------\n",
      "---  (train)  ---\n",
      "-----------------------------------\n",
      "Epoch 4/50 \n",
      "-----------------------------------\n",
      "---  (train)  ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-5af0d94931a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m G_update, D_update, E_update = train_model(\n\u001b[0;32m----> 3\u001b[0;31m     G, D, E, dataloader=train_dataloader, num_epochs=num_epochs)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-92-f6aa7f41eae8>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(G, D, E, dataloader, num_epochs)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m#　後半部分\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0minput_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mfake_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0md_out_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-18827e08a9e9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# 転置畳み込み層にいれるテンソルに形を変形する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    776\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    777\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs  = 50\n",
    "G_update, D_update, E_update = train_model(\n",
    "    G, D, E, dataloader=train_dataloader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
